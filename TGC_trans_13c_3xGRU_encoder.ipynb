{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing: Three layer GRU coders\n",
    "**Version 13c**:\n",
    "- Word level tokens\n",
    "- GRU type RNNs\n",
    "- 'sparse_categorical_crossentropy' to save memory\n",
    "- dropout to hinder overfitting\n",
    "- using large dataset\n",
    "- Two layer GRU encoder **new in this model**\n",
    "\n",
    "**Reference, best result so far:**\n",
    "- Model 13b after 65 epochs, 650 minutes: loss: 1.3777 - val_loss: 1.2907\n",
    "\n",
    "**Conclusions:**\n",
    "- Inference decoder not made yet!\n",
    "- Batch size makes a difference on loss, 512 seems to be too big on the small data set\n",
    "\n",
    "**Improvments to be implemented:**\n",
    "- clean-up code around the internal states, lots of confusion around \"[]\"\n",
    "- Larger latent_dim (=512?)\n",
    "- randomize input data?\n",
    "- try / understand 'TimeDistributed': decoder_dense = TimeDistributed(Dense(Y_lstm.shape[2], activation = 'relu'))\n",
    "- dropout as layer\n",
    "- L2 reg\n",
    "- deeper models to represent more complex sentences, more RNN layers?\n",
    "- bi-directional layers: https://stackoverflow.com/questions/50815354/seq2seq-bidirectional-encoder-decoder-in-keras\n",
    "- model.fit_generator to handle larger datasets\n",
    "- attention \n",
    "- Gradient clipping is important for RNNs training (clipvalue=1.0), book page 309\n",
    "- test: metrics=['accuracy']\n",
    "- set 'return_sequences' or 'return_stage' to false in models? Something is rotten\n",
    "- **Done** Smaller batch size, eg 32 vs 512 may give a getter val_loss? Proved to be that way with the chatbot!\n",
    "- **Done** test Pandas dataframe for nice outputs (https://www.tutorialspoint.com/python_pandas/python_pandas_dataframe.htm)\n",
    "- **Done** dropout in RNN layer:\n",
    "- **Done** Simplify by suing GRU RNN\n",
    "- **Done** ' to_categorical' as one-hot encoder, makes huge matrices\n",
    "- **Done** \"sparse_categorical_crossentropy\" to reduce the 'one hot' tensor\n",
    "- **Done** operates right now with long sentences: 8*std_div, shound be less when longer sentences are trained\n",
    "- **Done** train on larger dataset\n",
    "- **Done** something is wrong with the index of the one-hot; the model allows to return \"0\" as the best index, but the token2word starts from \"1\". It seems to be OK\n",
    "\n",
    "**Credits to many fine people on the internet:**\n",
    "- https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "- https://medium.com/@dev.elect.iitd/neural-machine-translation-using-word-level-seq2seq-model-47538cba8cd7\n",
    "- https://stackoverflow.com/questions/49477097/keras-seq2seq-word-embedding\n",
    "- https://github.com/devm2024/nmt_keras/blob/master/base.ipynb\n",
    "- https://www.kaggle.com/ievgenvp/lstm-encoder-decoder-via-keras-lb-0-5\n",
    "- https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/21_Machine_Translation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ThomasGordon\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, GRU, Dense\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.2-tf'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "\n",
    "num_samples = 100000       # Number of samples to train on\n",
    "num_words = 10000          # Limit vocabulary in translation\n",
    "latent_dim = 256           # Latent dimensionality of the encoding space\n",
    "\n",
    "batch_size = 32            # Batch size for training.\n",
    "numEpochs = 100            # Number of epochs to train for.\n",
    "DropOut = 0.3              # Used in GRU layers\n",
    "\n",
    "dataSetSize = 14839        # small dataset = 14839, all data = 9999999\n",
    "\n",
    "truncate_std_div = 2       # truncate sentences after x tokens, 2 standard deviations = 95% included\n",
    "mark_start = 'ssss '       # start and end markes for destination sentences\n",
    "mark_end = ' eeee'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read training data into tables\n",
    "the result is two tables with the input and output texts:input_texts[] and output_texts[]. Output texts are enriched bed adding the start and end markers. I join sentences from two datasets to get a larger and more diverse training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of small dataset:  14839\n",
      "['Aldrig i livet!', 'Ikke tale om!', 'Absolut ikke!', 'Under ingen omstændigheder!', 'Aldrig i verden!']\n",
      "['ssss No way! eeee', 'ssss No way! eeee', 'ssss No way! eeee', 'ssss No way! eeee', 'ssss No way! eeee']\n"
     ]
    }
   ],
   "source": [
    "# Read data into tables, first data set\n",
    "input_texts_a = []\n",
    "target_texts_a = []\n",
    "\n",
    "with open('dan-eng/dan.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    target_sentence, input_sentence = line.split('\\t')                   # reversed to get DA --> EN\n",
    "    target_sentence = mark_start + target_sentence.strip() + mark_end\n",
    "    input_texts_a.append(input_sentence)\n",
    "    target_texts_a.append(target_sentence)\n",
    "\n",
    "# Examples\n",
    "print('Size of small dataset: ', len(input_texts_a))\n",
    "print(input_texts_a[15:20])\n",
    "print(target_texts_a[15:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source into a table, the second and larger dataset\n",
    "filename = \"europarl-v7.da-en.da\"\n",
    "data_dir = \"data/europarl/\"\n",
    "path = os.path.join(data_dir, filename)\n",
    "with open(path, encoding=\"utf-8\") as file:\n",
    "    # Read the line from file, strip leading and trailing whitespace,\n",
    "    # prepend the start-text and append the end-text.\n",
    "    input_texts = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# destination into a table, the second and larger dataset\n",
    "filename = \"europarl-v7.da-en.en\"\n",
    "path = os.path.join(data_dir, filename)\n",
    "with open(path, encoding=\"utf-8\") as file:\n",
    "    # Read the line from file, strip leading and trailing whitespace,\n",
    "    # prepend the start-text and append the end-text.\n",
    "    target_texts = [mark_start + line.strip() + mark_end for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of large dataset:  1968800\n",
      "Som De kan se, indfandt det store \"år 2000-problem\" sig ikke. Til gengæld har borgerne i en del af medlemslandene været ramt af meget forfærdelige naturkatastrofer.\n",
      "ssss Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful. eeee\n"
     ]
    }
   ],
   "source": [
    "print('Size of large dataset: ', len(input_texts))\n",
    "i = 2\n",
    "print(input_texts[i])\n",
    "print(target_texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of small+large dataset:  1983639\n"
     ]
    }
   ],
   "source": [
    "# join the two data set to one big, gives me both short and long sentences\n",
    "input_texts = input_texts_a + input_texts\n",
    "target_texts = target_texts_a + target_texts\n",
    "print('Size of small+large dataset: ', len(input_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size:    1983639 1983639\n",
      "New lighter dataset size: 14839 14839\n"
     ]
    }
   ],
   "source": [
    "print\n",
    "print('Original dataset size:   ', len(input_texts), len(target_texts))\n",
    "input_texts = input_texts[:dataSetSize]\n",
    "target_texts = target_texts[:dataSetSize]\n",
    "print('New lighter dataset size:', len(input_texts), len(target_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source texts</th>\n",
       "      <th>Target texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12112</th>\n",
       "      <td>De behøver ikke at gå i skole i dag.</td>\n",
       "      <td>ssss They don't have to go to school today. eeee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6079</th>\n",
       "      <td>Kan I huske Tom?</td>\n",
       "      <td>ssss Do you guys remember Tom? eeee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9051</th>\n",
       "      <td>Kurven var fuld af æbler.</td>\n",
       "      <td>ssss The basket was full of apples. eeee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3747</th>\n",
       "      <td>Han har skrevet et brev til mig.</td>\n",
       "      <td>ssss He wrote me a letter. eeee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10312</th>\n",
       "      <td>Jeg har lidt penge i denne måned.</td>\n",
       "      <td>ssss I have a little money this month. eeee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>Du er en god mand.</td>\n",
       "      <td>ssss You're a good man. eeee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11148</th>\n",
       "      <td>Jeg takker jer på forhånd for jeres hjælp.</td>\n",
       "      <td>ssss Thank you in advance for your help. eeee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6690</th>\n",
       "      <td>Han blev dømt til døden.</td>\n",
       "      <td>ssss He was sentenced to death. eeee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3289</th>\n",
       "      <td>Jeg er ikke din far.</td>\n",
       "      <td>ssss I'm not your father. eeee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10364</th>\n",
       "      <td>I femten år har jeg været lærer.</td>\n",
       "      <td>ssss I've been a teacher for 15 years. eeee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Source texts  \\\n",
       "12112  De behøver ikke at gå i skole i dag.         \n",
       "6079   Kan I huske Tom?                             \n",
       "9051   Kurven var fuld af æbler.                    \n",
       "3747   Han har skrevet et brev til mig.             \n",
       "10312  Jeg har lidt penge i denne måned.            \n",
       "2438   Du er en god mand.                           \n",
       "11148  Jeg takker jer på forhånd for jeres hjælp.   \n",
       "6690   Han blev dømt til døden.                     \n",
       "3289   Jeg er ikke din far.                         \n",
       "10364  I femten år har jeg været lærer.             \n",
       "\n",
       "                                           Target texts  \n",
       "12112  ssss They don't have to go to school today. eeee  \n",
       "6079   ssss Do you guys remember Tom? eeee               \n",
       "9051   ssss The basket was full of apples. eeee          \n",
       "3747   ssss He wrote me a letter. eeee                   \n",
       "10312  ssss I have a little money this month. eeee       \n",
       "2438   ssss You're a good man. eeee                      \n",
       "11148  ssss Thank you in advance for your help. eeee     \n",
       "6690   ssss He was sentenced to death. eeee              \n",
       "3289   ssss I'm not your father. eeee                    \n",
       "10364  ssss I've been a teacher for 15 years. eeee       "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing Pandas\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "df = pd.DataFrame({'Source texts':input_texts, 'Target texts':target_texts})\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize input sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7068 unique source tokens.\n",
      "Longest sentence is 27 tokens.\n",
      "Sentences shortened to max 10 tokens.\n",
      "Shape of input tokens: (14839, 10)\n",
      "Input example:  [ 31 135   7   0   0   0   0   0   0   0]\n",
      "de købte det\n",
      "De købte det.\n",
      "[ 31 135   7   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# index to be used as demo\n",
    "idx = 1000\n",
    "\n",
    "# crate input tokenizer and create vocabulary from the texts\n",
    "tokenizer_inp = Tokenizer(num_words=num_words)\n",
    "tokenizer_inp.fit_on_texts(input_texts)\n",
    "print('Found %s unique source tokens.' % len(tokenizer_inp.word_index))\n",
    "\n",
    "# translate from word sentences to token sentences\n",
    "tokens_inp = tokenizer_inp.texts_to_sequences(input_texts)\n",
    "\n",
    "# Shorten the longest token sentences, Find the length of all sentences, truncate after x * std deviations\n",
    "num_tokens = [len(x) for x in tokens_inp]\n",
    "print('Longest sentence is %s tokens.' % max(num_tokens))\n",
    "max_tokens_input = np.mean(num_tokens) + truncate_std_div * np.std(num_tokens)\n",
    "max_tokens_input = min(int(max_tokens_input), max(num_tokens))\n",
    "print('Sentences shortened to max %s tokens.' % max_tokens_input)\n",
    "\n",
    "# Pad / truncate all token-sequences to the given length\n",
    "tokens_padded_input = pad_sequences(tokens_inp,\n",
    "                                    maxlen=max_tokens_input,\n",
    "                                    padding='post',\n",
    "                                    truncating='post')\n",
    "print('Shape of input tokens:', tokens_padded_input.shape)\n",
    "print('Input example: ', tokens_padded_input[idx])\n",
    "\n",
    "# Create inverse lookup from integer-tokens to words\n",
    "index_to_word_input = dict(zip(tokenizer_inp.word_index.values(), tokenizer_inp.word_index.keys()))\n",
    "\n",
    "# function to return readable text from tokens string\n",
    "def tokens_to_string_inp(tokens):\n",
    "    words = [index_to_word_input[token] \n",
    "            for token in tokens\n",
    "            if token != 0]\n",
    "    text = \" \".join(words)\n",
    "    return text\n",
    "\n",
    "# demo to show that it works\n",
    "print(tokens_to_string_inp(tokens_padded_input[idx]))\n",
    "print(input_texts[idx])\n",
    "print(tokens_padded_input[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize destination sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5295 unique target tokens.\n",
      "Longest sentence is 26 tokens.\n",
      "Sentences shortened to max 12 tokens.\n",
      "Shape of target tokens: (14839, 12)\n",
      "Target example:  [  1  49 116  14   2   0   0   0   0   0   0   0]\n",
      "not school tom\n",
      "ssss They bought it. eeee\n",
      "[  1  49 116  14   2   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# index to be used as demo\n",
    "idx = 1000\n",
    "\n",
    "# crate input tokenizer and create vocabulary from the texts\n",
    "tokenizer_target = Tokenizer(num_words=num_words)\n",
    "tokenizer_target.fit_on_texts(target_texts)\n",
    "print('Found %s unique target tokens.' % len(tokenizer_target.word_index))\n",
    "\n",
    "# translate from word sentences to token sentences\n",
    "tokens_target = tokenizer_target.texts_to_sequences(target_texts)\n",
    "\n",
    "# translate from word sentences to token sentences\n",
    "tokens_target = tokenizer_target.texts_to_sequences(target_texts)\n",
    "\n",
    "# Shorten the longest token sentences, Find the length of all sentences, truncate after x * std deviations\n",
    "num_tokens = [len(x) for x in tokens_target]\n",
    "print('Longest sentence is %s tokens.' % max(num_tokens))\n",
    "max_tokens_target = np.mean(num_tokens) + truncate_std_div * np.std(num_tokens)\n",
    "max_tokens_target = min(int(max_tokens_target), max(num_tokens))\n",
    "print('Sentences shortened to max %s tokens.' % max_tokens_target)\n",
    "\n",
    "# Pad / truncate all token-sequences to the given length\n",
    "tokens_padded_target = pad_sequences(tokens_target,\n",
    "                                     maxlen=max_tokens_target,\n",
    "                                     padding='post',\n",
    "                                    truncating='post')\n",
    "print('Shape of target tokens:', tokens_padded_target.shape)\n",
    "print('Target example: ', tokens_padded_target[idx])\n",
    "\n",
    "# Create inverse lookup from integer-tokens to words\n",
    "index_to_word_target = dict(zip(tokenizer_target.word_index.values(), tokenizer_target.word_index.keys()))\n",
    "\n",
    "# function to return readable text from tokens string\n",
    "def tokens_to_string_target(tokens):\n",
    "    words = [index_to_word_target[token] \n",
    "            for token in tokens\n",
    "            if token != 0]\n",
    "    text = \" \".join(words)\n",
    "    return text\n",
    "\n",
    "# demo to show that it works\n",
    "print(tokens_to_string_target(tokens_padded_input[idx]))\n",
    "print(target_texts[idx])\n",
    "print(tokens_padded_target[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n"
     ]
    }
   ],
   "source": [
    "# start and end marks as tokens, needed when translating\n",
    "token_start = tokenizer_target.word_index[mark_start.strip()]\n",
    "token_end = tokenizer_target.word_index[mark_end.strip()]\n",
    "print(token_start, token_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traing data\n",
    "- Input to the encoder is simply the source language as it is\n",
    "- Inputs to the decoder are slightly more complicated, since the two input strings are shiften one time-step: The model has to learn to predict the \"next\" token in the output from the input. Slizing is used to get two \"views\" to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14839, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data = tokens_padded_input\n",
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14839, 11)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data = tokens_padded_target[:, :-1]\n",
    "decoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14839, 11)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_data = tokens_padded_target[:, 1:]\n",
    "decoder_target_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples showing the training data to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 31, 135,   7,   0,   0,   0,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,  49, 116,  14,   2,   0,   0,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 49, 116,  14,   2,   0,   0,   0,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU encoder elements\n",
    "encoder_inputs = Input(shape=(None,), name='encoder_input')\n",
    "encoder_embed = Embedding(num_words, latent_dim, name='encoder_embedding')\n",
    "encoder_GRU1 = GRU(latent_dim, \n",
    "                   dropout=DropOut, \n",
    "                   recurrent_dropout=DropOut, \n",
    "                   return_sequences=True, \n",
    "                   name='encoder_gru1')\n",
    "encoder_GRU2 = GRU(latent_dim, \n",
    "                   dropout=DropOut, \n",
    "                   recurrent_dropout=DropOut, \n",
    "                   return_sequences=True, \n",
    "                   name='encoder_gru2')\n",
    "encoder_GRU3 = GRU(latent_dim, \n",
    "                   dropout=DropOut, \n",
    "                   recurrent_dropout=DropOut, \n",
    "                   return_state=True, \n",
    "                   name='encoder_gru3')\n",
    "\n",
    "# connect encoder\n",
    "net = encoder_inputs\n",
    "net = encoder_embed(net)\n",
    "net = encoder_GRU1(net)\n",
    "net = encoder_GRU2(net)\n",
    "net = encoder_GRU3(net)\n",
    "\n",
    "# outputs of encoder, from last GRU layer\n",
    "encoder_outputs, state_h = net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedding (Embedding)   (None, None, 256)    2560000     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_gru1 (GRU)              (None, None, 256)    393984      encoder_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_gru2 (GRU)              (None, None, 256)    393984      encoder_gru1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedding (Embedding)   (None, None, 256)    2560000     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_gru3 (GRU)              [(None, 256), (None, 393984      encoder_gru2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru1 (GRU)              (None, None, 256)    393984      decoder_embedding[0][0]          \n",
      "                                                                 encoder_gru3[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru2 (GRU)              (None, None, 256)    393984      decoder_gru1[0][0]               \n",
      "                                                                 encoder_gru3[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru3 (GRU)              [(None, None, 256),  393984      decoder_gru2[0][0]               \n",
      "                                                                 encoder_gru3[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_output (Dense)          (None, None, 10000)  2570000     decoder_gru3[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 10,053,904\n",
      "Trainable params: 10,053,904\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# GRU decoder elements, using `state_h` (encoder hidden states) as initial state\n",
    "decoder_inputs = Input(shape=(None,), name='decoder_input')\n",
    "decoder_embed =  Embedding(num_words, latent_dim, name='decoder_embedding')\n",
    "decoder_gru1 =   GRU(latent_dim, \n",
    "                     dropout=DropOut, recurrent_dropout=DropOut, \n",
    "                     return_sequences=True, \n",
    "                     return_state=False, \n",
    "                     name='decoder_gru1')    \n",
    "decoder_gru2 =   GRU(latent_dim, \n",
    "                     dropout=DropOut, recurrent_dropout=DropOut, \n",
    "                     return_sequences=True, \n",
    "                     return_state=False, \n",
    "                     name='decoder_gru2')    \n",
    "decoder_gru3 =   GRU(latent_dim, \n",
    "                     dropout=DropOut, recurrent_dropout=DropOut, \n",
    "                     return_sequences=True, \n",
    "                     return_state=True, \n",
    "                     name='decoder_gru3')    \n",
    "decoder_dense =  Dense(num_words, activation='linear', name='decoder_output')\n",
    "\n",
    "# connect decoder\n",
    "net = decoder_inputs\n",
    "net = decoder_embed(net)\n",
    "net = decoder_gru1(net, initial_state=state_h)\n",
    "net = decoder_gru2(net, initial_state=state_h)\n",
    "net = decoder_gru3(net, initial_state=state_h)\n",
    "\n",
    "# connect dense layer to GRUs\n",
    "decoder_outputs, dec_states_h = net\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# define decoder model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"719pt\" viewBox=\"0.00 0.00 771.50 719.00\" width=\"772pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 715)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-715 767.5,-715 767.5,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 1427806559648 -->\n",
       "<g class=\"node\" id=\"node1\"><title>1427806559648</title>\n",
       "<polygon fill=\"none\" points=\"418,-664.5 418,-710.5 731,-710.5 731,-664.5 418,-664.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"500\" y=\"-683.8\">encoder_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"582,-664.5 582,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"610\" y=\"-695.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"582,-687.5 638,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"610\" y=\"-672.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"638,-664.5 638,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"684.5\" y=\"-695.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"638,-687.5 731,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"684.5\" y=\"-672.3\">(None, None)</text>\n",
       "</g>\n",
       "<!-- 1427806556736 -->\n",
       "<g class=\"node\" id=\"node2\"><title>1427806556736</title>\n",
       "<polygon fill=\"none\" points=\"385.5,-581.5 385.5,-627.5 763.5,-627.5 763.5,-581.5 385.5,-581.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"486\" y=\"-600.8\">encoder_embedding: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"586.5,-581.5 586.5,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"614.5\" y=\"-612.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"586.5,-604.5 642.5,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"614.5\" y=\"-589.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"642.5,-581.5 642.5,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"703\" y=\"-612.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"642.5,-604.5 763.5,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"703\" y=\"-589.3\">(None, None, 256)</text>\n",
       "</g>\n",
       "<!-- 1427806559648&#45;&gt;1427806556736 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>1427806559648-&gt;1427806556736</title>\n",
       "<path d=\"M574.5,-664.366C574.5,-656.152 574.5,-646.658 574.5,-637.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"578,-637.607 574.5,-627.607 571,-637.607 578,-637.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1427395851040 -->\n",
       "<g class=\"node\" id=\"node3\"><title>1427395851040</title>\n",
       "<polygon fill=\"none\" points=\"420.5,-498.5 420.5,-544.5 728.5,-544.5 728.5,-498.5 420.5,-498.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"486\" y=\"-517.8\">encoder_gru1: GRU</text>\n",
       "<polyline fill=\"none\" points=\"551.5,-498.5 551.5,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"579.5\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"551.5,-521.5 607.5,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"579.5\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"607.5,-498.5 607.5,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"668\" y=\"-529.3\">(None, None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"607.5,-521.5 728.5,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"668\" y=\"-506.3\">(None, None, 256)</text>\n",
       "</g>\n",
       "<!-- 1427806556736&#45;&gt;1427395851040 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>1427806556736-&gt;1427395851040</title>\n",
       "<path d=\"M574.5,-581.366C574.5,-573.152 574.5,-563.658 574.5,-554.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"578,-554.607 574.5,-544.607 571,-554.607 578,-554.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1427806559200 -->\n",
       "<g class=\"node\" id=\"node5\"><title>1427806559200</title>\n",
       "<polygon fill=\"none\" points=\"420.5,-415.5 420.5,-461.5 728.5,-461.5 728.5,-415.5 420.5,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"486\" y=\"-434.8\">encoder_gru2: GRU</text>\n",
       "<polyline fill=\"none\" points=\"551.5,-415.5 551.5,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"579.5\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"551.5,-438.5 607.5,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"579.5\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"607.5,-415.5 607.5,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"668\" y=\"-446.3\">(None, None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"607.5,-438.5 728.5,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"668\" y=\"-423.3\">(None, None, 256)</text>\n",
       "</g>\n",
       "<!-- 1427395851040&#45;&gt;1427806559200 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>1427395851040-&gt;1427806559200</title>\n",
       "<path d=\"M574.5,-498.366C574.5,-490.152 574.5,-480.658 574.5,-471.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"578,-471.607 574.5,-461.607 571,-471.607 578,-471.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1425942565048 -->\n",
       "<g class=\"node\" id=\"node4\"><title>1425942565048</title>\n",
       "<polygon fill=\"none\" points=\"32.5,-415.5 32.5,-461.5 346.5,-461.5 346.5,-415.5 32.5,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"115\" y=\"-434.8\">decoder_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"197.5,-415.5 197.5,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"225.5\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"197.5,-438.5 253.5,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"225.5\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"253.5,-415.5 253.5,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300\" y=\"-446.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"253.5,-438.5 346.5,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300\" y=\"-423.3\">(None, None)</text>\n",
       "</g>\n",
       "<!-- 1425942565104 -->\n",
       "<g class=\"node\" id=\"node6\"><title>1425942565104</title>\n",
       "<polygon fill=\"none\" points=\"0,-332.5 0,-378.5 379,-378.5 379,-332.5 0,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"101\" y=\"-351.8\">decoder_embedding: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"202,-332.5 202,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"230\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"202,-355.5 258,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"230\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"258,-332.5 258,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"318.5\" y=\"-363.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"258,-355.5 379,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"318.5\" y=\"-340.3\">(None, None, 256)</text>\n",
       "</g>\n",
       "<!-- 1425942565048&#45;&gt;1425942565104 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>1425942565048-&gt;1425942565104</title>\n",
       "<path d=\"M189.5,-415.366C189.5,-407.152 189.5,-397.658 189.5,-388.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"193,-388.607 189.5,-378.607 186,-388.607 193,-388.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1427806559144 -->\n",
       "<g class=\"node\" id=\"node7\"><title>1427806559144</title>\n",
       "<polygon fill=\"none\" points=\"397.5,-332.5 397.5,-378.5 751.5,-378.5 751.5,-332.5 397.5,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"463\" y=\"-351.8\">encoder_gru3: GRU</text>\n",
       "<polyline fill=\"none\" points=\"528.5,-332.5 528.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"556.5\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"528.5,-355.5 584.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"556.5\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"584.5,-332.5 584.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"668\" y=\"-363.3\">(None, None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"584.5,-355.5 751.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"668\" y=\"-340.3\">[(None, 256), (None, 256)]</text>\n",
       "</g>\n",
       "<!-- 1427806559200&#45;&gt;1427806559144 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>1427806559200-&gt;1427806559144</title>\n",
       "<path d=\"M574.5,-415.366C574.5,-407.152 574.5,-397.658 574.5,-388.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"578,-388.607 574.5,-378.607 571,-388.607 578,-388.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1425942565160 -->\n",
       "<g class=\"node\" id=\"node8\"><title>1425942565160</title>\n",
       "<polygon fill=\"none\" points=\"155,-249.5 155,-295.5 546,-295.5 546,-249.5 155,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"220.5\" y=\"-268.8\">decoder_gru1: GRU</text>\n",
       "<polyline fill=\"none\" points=\"286,-249.5 286,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"286,-272.5 342,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"342,-249.5 342,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"444\" y=\"-280.3\">[(None, None, 256), (None, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"342,-272.5 546,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"444\" y=\"-257.3\">(None, None, 256)</text>\n",
       "</g>\n",
       "<!-- 1425942565104&#45;&gt;1425942565160 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>1425942565104-&gt;1425942565160</title>\n",
       "<path d=\"M233.494,-332.366C253.283,-322.41 276.806,-310.576 297.552,-300.138\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"299.199,-303.228 306.559,-295.607 296.053,-296.975 299.199,-303.228\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1427806559144&#45;&gt;1425942565160 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>1427806559144-&gt;1425942565160</title>\n",
       "<path d=\"M513.587,-332.473C484.946,-322.117 450.633,-309.709 420.886,-298.952\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"421.995,-295.631 411.401,-295.522 419.615,-302.214 421.995,-295.631\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1425942565328 -->\n",
       "<g class=\"node\" id=\"node9\"><title>1425942565328</title>\n",
       "<polygon fill=\"none\" points=\"155,-166.5 155,-212.5 546,-212.5 546,-166.5 155,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"220.5\" y=\"-185.8\">decoder_gru2: GRU</text>\n",
       "<polyline fill=\"none\" points=\"286,-166.5 286,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"286,-189.5 342,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"342,-166.5 342,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"444\" y=\"-197.3\">[(None, None, 256), (None, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"342,-189.5 546,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"444\" y=\"-174.3\">(None, None, 256)</text>\n",
       "</g>\n",
       "<!-- 1427806559144&#45;&gt;1425942565328 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>1427806559144-&gt;1425942565328</title>\n",
       "<path d=\"M574.5,-271.5C561.869,-246.418 540.176,-228.943 515.118,-216.804\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"516.299,-213.497 505.745,-212.57 513.417,-219.876 516.299,-213.497\" stroke=\"black\"/>\n",
       "<path d=\"M579.673,-332.293C582.453,-315.279 583.671,-291.712 574.5,-273.5\" fill=\"none\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1425942566056 -->\n",
       "<g class=\"node\" id=\"node10\"><title>1425942566056</title>\n",
       "<polygon fill=\"none\" points=\"267,-83.5 267,-129.5 658,-129.5 658,-83.5 267,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"332.5\" y=\"-102.8\">decoder_gru3: GRU</text>\n",
       "<polyline fill=\"none\" points=\"398,-83.5 398,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"426\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"398,-106.5 454,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"426\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"454,-83.5 454,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"556\" y=\"-114.3\">[(None, None, 256), (None, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"454,-106.5 658,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"556\" y=\"-91.3\">[(None, None, 256), (None, 256)]</text>\n",
       "</g>\n",
       "<!-- 1427806559144&#45;&gt;1425942566056 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>1427806559144-&gt;1425942566056</title>\n",
       "<path d=\"M574.5,-271.5C553.071,-228.948 582.155,-205.489 555.5,-166 547.176,-153.668 535.291,-143.42 522.788,-135.131\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"524.408,-132.017 514.069,-129.704 520.709,-137.96 524.408,-132.017\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1425942565160&#45;&gt;1425942565328 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>1425942565160-&gt;1425942565328</title>\n",
       "<path d=\"M350.5,-249.366C350.5,-241.152 350.5,-231.658 350.5,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"354,-222.607 350.5,-212.607 347,-222.607 354,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1425942565328&#45;&gt;1425942566056 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>1425942565328-&gt;1425942566056</title>\n",
       "<path d=\"M381.104,-166.366C394.192,-156.902 409.626,-145.739 423.518,-135.693\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"425.881,-138.303 431.933,-129.607 421.778,-132.631 425.881,-138.303\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1425942566616 -->\n",
       "<g class=\"node\" id=\"node11\"><title>1425942566616</title>\n",
       "<polygon fill=\"none\" points=\"294,-0.5 294,-46.5 631,-46.5 631,-0.5 294,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"367.5\" y=\"-19.8\">decoder_output: Dense</text>\n",
       "<polyline fill=\"none\" points=\"441,-0.5 441,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"469\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"441,-23.5 497,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"469\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"497,-0.5 497,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"564\" y=\"-31.3\">(None, None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"497,-23.5 631,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"564\" y=\"-8.3\">(None, None, 10000)</text>\n",
       "</g>\n",
       "<!-- 1425942566056&#45;&gt;1425942566616 -->\n",
       "<g class=\"edge\" id=\"edge12\"><title>1425942566056-&gt;1425942566616</title>\n",
       "<path d=\"M462.5,-83.3664C462.5,-75.1516 462.5,-65.6579 462.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"466,-56.6068 462.5,-46.6068 459,-56.6069 466,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualise model as a graph\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import pydot_ng as pydot\n",
    "import graphviz as graphviz\n",
    "SVG(model_to_dot(model,show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss function since sparse does not work: https://github.com/tensorflow/tensorflow/issues/17150\n",
    "def sparse_cross_entropy(y_true, y_pred):\n",
    "    # Calculate the loss. This outputs a 2-rank tensor of shape [batch_size, sequence_length]\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "    loss_mean = tf.reduce_mean(loss)\n",
    "    return loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_target = tf.placeholder(dtype='int32', shape=(None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss=sparse_cross_entropy,\n",
    "              target_tensors=[decoder_target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call backs to stop model when it does not improve more\n",
    "path_checkpoint = 'tgc_checkpoint.keras'\n",
    "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=1,\n",
    "                                      save_weights_only=True,\n",
    "                                      save_best_only=True)\n",
    "\n",
    "callback_early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                                        patience=3, \n",
    "                                        verbose=1)\n",
    "\n",
    "callbacks = [callback_early_stopping,\n",
    "             callback_checkpoint]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# load check-point if it exists (contunie training)\n",
    "try:\n",
    "    model.load_weights(path_checkpoint)\n",
    "except Exception as error:\n",
    "    print(\"Error trying to load checkpoint.\")\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Note that `decoder_target_data` needs to be one-hot encoded,\n",
    "# rather than sequences of integers like `decoder_input_data`!\n",
    "history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=numEpochs,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=callbacks)\n",
    "model.save('TGC_trans.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotter historikken for 'loss'\n",
    "loss     = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss)+1)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')       # bo = \"blue dot\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')  # b  = \"solid blue line\"\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference mode = testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create sampling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# encoder model used to create internal representation / states\n",
    "encoder_model = Model(encoder_inputs, state_h)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise encoder model as a graph\n",
    "SVG(model_to_dot(encoder_model,show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Graph disconnected: cannot obtain value for tensor Tensor(\"encoder_input:0\", shape=(?, ?), dtype=float32) at layer \"encoder_input\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-1687086ada02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m decoder_model = Model([decoder_inputs]+[decoder_state_input_h], \n\u001b[1;32m---> 25\u001b[1;33m                       [decoder_outputs]+[state_h])               # notice the '+' operator requires [] to work\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[0;32m   1809\u001b[0m                                 \u001b[1;34m'The following previous layers '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1810\u001b[0m                                 \u001b[1;34m'were accessed without issue: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1811\u001b[1;33m                                 str(layers_with_complete_input))\n\u001b[0m\u001b[0;32m   1812\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1813\u001b[0m                         \u001b[0mcomputable_tensors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Graph disconnected: cannot obtain value for tensor Tensor(\"encoder_input:0\", shape=(?, ?), dtype=float32) at layer \"encoder_input\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "#################################\n",
    "'''her skal bruges en ny initial state h'''\n",
    "\n",
    "\n",
    "decoder_initial_state = Input(shape=(latent_dim,),\n",
    "                              name='decoder_initial_state')\n",
    "\n",
    "\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "\n",
    "# connect decoder\n",
    "net2 = decoder_inputs\n",
    "net2 = decoder_embed(net2)\n",
    "net2 = decoder_gru1(net2, initial_state=decoder_initial_state)\n",
    "net2 = decoder_gru2(net2, initial_state=decoder_initial_state)\n",
    "net2 = decoder_gru3(net2, initial_state=decoder_initial_state)\n",
    "\n",
    "# connect dense layer to GRUs\n",
    "decoder_outputs, dec_states_h = net\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "decoder_model = Model([decoder_inputs]+[decoder_state_input_h], \n",
    "                      [decoder_outputs]+[state_h])               # notice the '+' operator requires [] to work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_inputs = [decoder_state_input_h,]\n",
    "\n",
    "# reuse the decoder we have trained\n",
    "decoder_embed_final2 = decoder_embed(decoder_inputs)\n",
    "\n",
    "decoder_outputs2, state_h2 = decoder_gru1(decoder_embed_final2, initial_state=decoder_state_inputs)\n",
    "\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_state_inputs, \n",
    "    [decoder_outputs2] + [state_h2])                    # notice the '+' operator requires [] to work !!!\n",
    "\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise decoder model as a graph\n",
    "SVG(model_to_dot(decoder_model,show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "   \n",
    "    # tokenize the text to be translated, and reverse\n",
    "    input_tokens = tokenizer_inp.texts_to_sequences([input_seq])\n",
    "    input_tokens = pad_sequences(input_tokens,\n",
    "                                 maxlen=max_tokens_input,\n",
    "                                 padding='post',\n",
    "                                 truncating='post')\n",
    "\n",
    "    # encode the input sentence\n",
    "    states_value = encoder_model.predict(input_tokens)\n",
    "    \n",
    "    # Generate empty target sequence of length 1 and insert start token\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = token_start    #\n",
    "\n",
    "    # sampling loop to generate translated words using decoder, word by word\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    while not stop_condition:  \n",
    "        # predict one next word, decoder returns probabilities for all words/tokens\n",
    "        output_tokens, h = decoder_model.predict([target_seq] + [states_value])\n",
    "        \n",
    "        # pick most probable token / word\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = index_to_word_target[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_word\n",
    "        \n",
    "        # Exit condition: either hit max length or find stop character.\n",
    "        if (sampled_word == 'eeee' or len(decoded_sentence) > 52):\n",
    "            stop_condition = True\n",
    "        \n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        \n",
    "        # Update states, so they can be re-injected in next token/word prediction\n",
    "        states_value = h\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing translation ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing on known sentences from training data\n",
    "for idx in range(1000, 1010):\n",
    "    input_seq = input_texts[idx]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(input_seq, '\\n ==> ', decoded_sentence, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing on unknown sentences from validation data\n",
    "for idx in range(12000, 12010):\n",
    "    input_seq = input_texts[idx]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(input_seq, '\\n ==> ', decoded_sentence, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = 'vi ses'\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print(input_seq, '\\n ==> ', decoded_sentence, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = 'gå nu'\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print(input_seq, '\\n ==> ', decoded_sentence, '\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
