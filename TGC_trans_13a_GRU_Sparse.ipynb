{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working model\n",
    "**Version 13a**:\n",
    "- Word level tokens\n",
    "- GRU type RNNs\n",
    "- 'sparse_categorical_crossentropy' to save memory\n",
    "- dropout to hinder overfitting\n",
    "\n",
    "**Conclusions:**\n",
    "- 'sparse' works!\n",
    "- 'sparse' runs 6x faster, strange, perhaps less work on fewer data?\n",
    "- testing 'dropout', works soso\n",
    "- 'so so' translation, perfect on training data, bad on validation data\n",
    "\n",
    "**Improvments to be implemented:**\n",
    "- randomize input data?\n",
    "- try / understand 'TimeDistributed': decoder_dense = TimeDistributed(Dense(Y_lstm.shape[2], activation = 'relu'))\n",
    "- **Done** dropout in RNN layer:\n",
    "- dropout as layer\n",
    "- L2 reg\n",
    "- **Done** Simplify by suing GRU RNN\n",
    "- **Done** ' to_categorical' as one-hot encoder, makes huge matrices\n",
    "- **Done** \"sparse_categorical_crossentropy\" to reduce the 'one hot' tensor\n",
    "- operates right now with long sentences: 8*std_div, shound be less when longer sentences are trained\n",
    "- deeper models to represent more complex sentences, more RNN layers?\n",
    "- bi-directional layers: https://stackoverflow.com/questions/50815354/seq2seq-bidirectional-encoder-decoder-in-keras\n",
    "- train on larger dataset\n",
    "- model.fit_generator to handle larger datasets\n",
    "- attention \n",
    "- Gradient clipping is important for RNNs training (clipvalue=1.0), book page 309\n",
    "- test: metrics=['accuracy']\n",
    "- **Done** something is wrong with the index of the one-hot; the model allows to return \"0\" as the best index, but the token2word starts from \"1\". It seems to be OK\n",
    "- set 'return_sequences' or 'return_stage' to false in models? Something is rotten\n",
    "\n",
    "**Credits to many fine people on the internet:**\n",
    "- https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "- https://medium.com/@dev.elect.iitd/neural-machine-translation-using-word-level-seq2seq-model-47538cba8cd7\n",
    "- https://stackoverflow.com/questions/49477097/keras-seq2seq-word-embedding\n",
    "- https://github.com/devm2024/nmt_keras/blob/master/base.ipynb\n",
    "- https://www.kaggle.com/ievgenvp/lstm-encoder-decoder-via-keras-lb-0-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ThomasGordon\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, GRU, Dense\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.2-tf'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "\n",
    "num_samples = 100000       # Number of samples to train on\n",
    "num_words = 10000          # Limit vocabulary in translation\n",
    "latent_dim = 256           # Latent dimensionality of the encoding space\n",
    "\n",
    "batch_size = 512           # Batch size for training.\n",
    "numEpochs = 200            # Number of epochs to train for.\n",
    "DropOut = 0.4              # Used in GRU layers\n",
    "\n",
    "truncate_std_div = 99      # truncate sentences after x tokens\n",
    "mark_start = 'ssss '       # start and end markes for destination sentences\n",
    "mark_end = ' eeee'\n",
    "\n",
    "data_path = 'dan-eng/dan.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read training data into tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No way!', 'No way!', 'No way!', 'No way!', 'No way!']\n",
      "['ssss Aldrig i livet! eeee', 'ssss Ikke tale om! eeee', 'ssss Absolut ikke! eeee', 'ssss Under ingen omstændigheder! eeee', 'ssss Aldrig i verden! eeee']\n"
     ]
    }
   ],
   "source": [
    "# Read data into tables\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_sentence, target_sentence = line.split('\\t')\n",
    "    target_sentence = mark_start + target_sentence.strip() + mark_end\n",
    "    input_texts.append(input_sentence)\n",
    "    target_texts.append(target_sentence)\n",
    "\n",
    "# Examples\n",
    "print(input_texts[15:20])\n",
    "print(target_texts[15:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize input sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5293 unique source tokens.\n",
      "Longest sentence is 24 tokens.\n",
      "Sentences shortened to max 24 tokens.\n",
      "Shape of input tokens: (14839, 24)\n",
      "Input example:  [   1   73    6   29  531 1326  360    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n",
      "the apple is not quite ripe yet\n",
      "The apple is not quite ripe yet.\n",
      "[   1   73    6   29  531 1326  360    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# crate input tokenizer and create vocabulary from the texts\n",
    "tokenizer_inp = Tokenizer(num_words=num_words)\n",
    "tokenizer_inp.fit_on_texts(input_texts)\n",
    "print('Found %s unique source tokens.' % len(tokenizer_inp.word_index))\n",
    "\n",
    "# translate from word sentences to token sentences\n",
    "tokens_inp = tokenizer_inp.texts_to_sequences(input_texts)\n",
    "\n",
    "# Shorten the longest token sentences, Find the length of all sentences, truncate after x * std deviations\n",
    "num_tokens = [len(x) for x in tokens_inp]\n",
    "print('Longest sentence is %s tokens.' % max(num_tokens))\n",
    "max_tokens_input = np.mean(num_tokens) + truncate_std_div * np.std(num_tokens)\n",
    "max_tokens_input = min(int(max_tokens_input), max(num_tokens))\n",
    "print('Sentences shortened to max %s tokens.' % max_tokens_input)\n",
    "\n",
    "# Pad / truncate all token-sequences to the given length\n",
    "tokens_padded_input = pad_sequences(tokens_inp,\n",
    "                                    maxlen=max_tokens_input,\n",
    "                                    padding='post',\n",
    "                                    truncating='post')\n",
    "print('Shape of input tokens:', tokens_padded_input.shape)\n",
    "print('Input example: ', tokens_padded_input[10000])\n",
    "\n",
    "# Create inverse lookup from integer-tokens to words\n",
    "index_to_word_input = dict(zip(tokenizer_inp.word_index.values(), tokenizer_inp.word_index.keys()))\n",
    "\n",
    "# function to return readable text from tokens string\n",
    "def tokens_to_string_inp(tokens):\n",
    "    words = [index_to_word_input[token] \n",
    "            for token in tokens\n",
    "            if token != 0]\n",
    "    text = \" \".join(words)\n",
    "    return text\n",
    "\n",
    "# demo to show that it works\n",
    "idx = 10000\n",
    "print(tokens_to_string_inp(tokens_padded_input[idx]))\n",
    "print(input_texts[idx])\n",
    "print(tokens_padded_input[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize destination sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7070 unique target tokens.\n",
      "Longest sentence is 29 tokens.\n",
      "Sentences shortened to max 29 tokens.\n",
      "Shape of target tokens: (14839, 29)\n",
      "Target example:  [   1  308    3   10  194 2333  273    2    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0]\n",
      "ssss tror at den problemer faldet 30\n",
      "ssss Æblet er ikke helt modent endnu. eeee\n",
      "[   1  308    3   10  194 2333  273    2    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0]\n"
     ]
    }
   ],
   "source": [
    "# crate input tokenizer and create vocabulary from the texts\n",
    "tokenizer_target = Tokenizer(num_words=num_words)\n",
    "tokenizer_target.fit_on_texts(target_texts)\n",
    "print('Found %s unique target tokens.' % len(tokenizer_target.word_index))\n",
    "\n",
    "# translate from word sentences to token sentences\n",
    "tokens_target = tokenizer_target.texts_to_sequences(target_texts)\n",
    "\n",
    "# translate from word sentences to token sentences\n",
    "tokens_target = tokenizer_target.texts_to_sequences(target_texts)\n",
    "\n",
    "# Shorten the longest token sentences, Find the length of all sentences, truncate after x * std deviations\n",
    "num_tokens = [len(x) for x in tokens_target]\n",
    "print('Longest sentence is %s tokens.' % max(num_tokens))\n",
    "max_tokens_target = np.mean(num_tokens) + truncate_std_div * np.std(num_tokens)\n",
    "max_tokens_target = min(int(max_tokens_target), max(num_tokens))\n",
    "print('Sentences shortened to max %s tokens.' % max_tokens_target)\n",
    "\n",
    "# Pad / truncate all token-sequences to the given length\n",
    "tokens_padded_target = pad_sequences(tokens_target,\n",
    "                                     maxlen=max_tokens_target,\n",
    "                                     padding='post',\n",
    "                                    truncating='post')\n",
    "print('Shape of target tokens:', tokens_padded_target.shape)\n",
    "print('Target example: ', tokens_padded_target[10000])\n",
    "\n",
    "# Create inverse lookup from integer-tokens to words\n",
    "index_to_word_target = dict(zip(tokenizer_target.word_index.values(), tokenizer_target.word_index.keys()))\n",
    "\n",
    "# function to return readable text from tokens string\n",
    "def tokens_to_string_target(tokens):\n",
    "    words = [index_to_word_target[token] \n",
    "            for token in tokens\n",
    "            if token != 0]\n",
    "    text = \" \".join(words)\n",
    "    return text\n",
    "\n",
    "# demo to show that it works\n",
    "idx = 10000\n",
    "print(tokens_to_string_target(tokens_padded_input[idx]))\n",
    "print(target_texts[idx])\n",
    "print(tokens_padded_target[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n"
     ]
    }
   ],
   "source": [
    "# start and end marks as tokens, needed when translating\n",
    "token_start = tokenizer_target.word_index[mark_start.strip()]\n",
    "token_end = tokenizer_target.word_index[mark_end.strip()]\n",
    "print(token_start, token_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traing data\n",
    "- Input to the encoder is simply the source language as it is\n",
    "- Inputs to the decoder are slightly more complicated, since the two input strings are shiften one time-step: The model has to learn to predict the \"next\" token in the output from the input. Slizing is used to get two \"views\" to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14839, 24)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data = tokens_padded_input\n",
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14839, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data = tokens_padded_target[:, :-1]\n",
    "decoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14839, 28)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_data = tokens_padded_target[:, 1:]\n",
    "decoder_target_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples showing the training data to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,   73,    6,   29,  531, 1326,  360,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,  308,    3,   10,  194, 2333,  273,    2,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 308,    3,   10,  194, 2333,  273,    2,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encode 'decoder_target_data' since this is what the decoder produces as output"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "decoder_target_data_onehot = np.zeros((decoder_target_data.shape[0], decoder_target_data.shape[1], num_words), dtype=bool)\n",
    "for i, sentence in enumerate(decoder_target_data):\n",
    "    for j, token in enumerate(sentence):\n",
    "        decoder_target_data_onehot[i, j, token] = 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# worried about the menory usage ...\n",
    "from sys import getsizeof\n",
    "SizeInMB = getsizeof(decoder_target_data_onehot) / 1024 / 1024\n",
    "print('Reality check: Size of one-hot tensor in MB:', SizeInMB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "\n",
    "encoder_embed = Embedding(num_words, latent_dim)\n",
    "encoder_embed_final = encoder_embed(encoder_inputs)\n",
    "\n",
    "encoder = GRU(latent_dim, dropout=DropOut, recurrent_dropout=DropOut, return_state=True)\n",
    "encoder_outputs, state_h = encoder(encoder_embed_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    2560000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 256)    2560000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     [(None, 256), (None, 393984      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     [(None, None, 256),  393984      embedding_2[0][0]                \n",
      "                                                                 gru_1[0][1]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 10000)  2570000     gru_2[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 8,477,968\n",
      "Trainable params: 8,477,968\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Set up GRU decoder, using `encoder_states` as initial state\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "decoder_embed = Embedding(num_words, latent_dim)\n",
    "decoder_embed_final = decoder_embed(decoder_inputs)\n",
    "\n",
    "decoder_gru = GRU(latent_dim, dropout=DropOut, recurrent_dropout=DropOut, return_sequences=True, return_state=True)            \n",
    "decoder_outputs, dec_states_h = decoder_gru(decoder_embed_final, initial_state=state_h)\n",
    "\n",
    "decoder_dense = Dense(num_words, activation='linear')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"387pt\" viewBox=\"0.00 0.00 691.00 387.00\" width=\"691pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 383)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-383 687,-383 687,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 2750309146352 -->\n",
       "<g class=\"node\" id=\"node1\"><title>2750309146352</title>\n",
       "<polygon fill=\"none\" points=\"32.5,-332.5 32.5,-378.5 307.5,-378.5 307.5,-332.5 32.5,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"95.5\" y=\"-351.8\">input_1: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"158.5,-332.5 158.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"186.5\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"158.5,-355.5 214.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"186.5\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"214.5,-332.5 214.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"261\" y=\"-363.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"214.5,-355.5 307.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"261\" y=\"-340.3\">(None, None)</text>\n",
       "</g>\n",
       "<!-- 2750309143104 -->\n",
       "<g class=\"node\" id=\"node3\"><title>2750309143104</title>\n",
       "<polygon fill=\"none\" points=\"0,-249.5 0,-295.5 340,-295.5 340,-249.5 0,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-268.8\">embedding_1: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"163,-249.5 163,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"191\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"163,-272.5 219,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"191\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"219,-249.5 219,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"279.5\" y=\"-280.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"219,-272.5 340,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"279.5\" y=\"-257.3\">(None, None, 256)</text>\n",
       "</g>\n",
       "<!-- 2750309146352&#45;&gt;2750309143104 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>2750309146352-&gt;2750309143104</title>\n",
       "<path d=\"M170,-332.366C170,-324.152 170,-314.658 170,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"173.5,-305.607 170,-295.607 166.5,-305.607 173.5,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2750321578672 -->\n",
       "<g class=\"node\" id=\"node2\"><title>2750321578672</title>\n",
       "<polygon fill=\"none\" points=\"375.5,-249.5 375.5,-295.5 650.5,-295.5 650.5,-249.5 375.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"438.5\" y=\"-268.8\">input_2: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"501.5,-249.5 501.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"529.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"501.5,-272.5 557.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"529.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"557.5,-249.5 557.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"604\" y=\"-280.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"557.5,-272.5 650.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"604\" y=\"-257.3\">(None, None)</text>\n",
       "</g>\n",
       "<!-- 2750321578616 -->\n",
       "<g class=\"node\" id=\"node4\"><title>2750321578616</title>\n",
       "<polygon fill=\"none\" points=\"343,-166.5 343,-212.5 683,-212.5 683,-166.5 343,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"424.5\" y=\"-185.8\">embedding_2: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"506,-166.5 506,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"534\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"506,-189.5 562,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"534\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"562,-166.5 562,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"622.5\" y=\"-197.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"562,-189.5 683,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"622.5\" y=\"-174.3\">(None, None, 256)</text>\n",
       "</g>\n",
       "<!-- 2750321578672&#45;&gt;2750321578616 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>2750321578672-&gt;2750321578616</title>\n",
       "<path d=\"M513,-249.366C513,-241.152 513,-231.658 513,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"516.5,-222.607 513,-212.607 509.5,-222.607 516.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2750309143384 -->\n",
       "<g class=\"node\" id=\"node5\"><title>2750309143384</title>\n",
       "<polygon fill=\"none\" points=\"15.5,-166.5 15.5,-212.5 324.5,-212.5 324.5,-166.5 15.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"58.5\" y=\"-185.8\">gru_1: GRU</text>\n",
       "<polyline fill=\"none\" points=\"101.5,-166.5 101.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"129.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"101.5,-189.5 157.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"129.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"157.5,-166.5 157.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"241\" y=\"-197.3\">(None, None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"157.5,-189.5 324.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"241\" y=\"-174.3\">[(None, 256), (None, 256)]</text>\n",
       "</g>\n",
       "<!-- 2750309143104&#45;&gt;2750309143384 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>2750309143104-&gt;2750309143384</title>\n",
       "<path d=\"M170,-249.366C170,-241.152 170,-231.658 170,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"173.5,-222.607 170,-212.607 166.5,-222.607 173.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2750321578952 -->\n",
       "<g class=\"node\" id=\"node6\"><title>2750321578952</title>\n",
       "<polygon fill=\"none\" points=\"168,-83.5 168,-129.5 514,-129.5 514,-83.5 168,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"211\" y=\"-102.8\">gru_2: GRU</text>\n",
       "<polyline fill=\"none\" points=\"254,-83.5 254,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"282\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"254,-106.5 310,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"282\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"310,-83.5 310,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"412\" y=\"-114.3\">[(None, None, 256), (None, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"310,-106.5 514,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"412\" y=\"-91.3\">[(None, None, 256), (None, 256)]</text>\n",
       "</g>\n",
       "<!-- 2750321578616&#45;&gt;2750321578952 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>2750321578616-&gt;2750321578952</title>\n",
       "<path d=\"M466,-166.366C444.764,-156.366 419.504,-144.47 397.267,-133.998\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"398.481,-130.701 387.943,-129.607 395.499,-137.034 398.481,-130.701\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2750309143384&#45;&gt;2750321578952 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>2750309143384-&gt;2750321578952</title>\n",
       "<path d=\"M216.727,-166.366C237.839,-156.366 262.953,-144.47 285.06,-133.998\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"286.791,-137.051 294.33,-129.607 283.794,-130.725 286.791,-137.051\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2750321579568 -->\n",
       "<g class=\"node\" id=\"node7\"><title>2750321579568</title>\n",
       "<polygon fill=\"none\" points=\"194,-0.5 194,-46.5 488,-46.5 488,-0.5 194,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"246\" y=\"-19.8\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"298,-0.5 298,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"326\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"298,-23.5 354,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"326\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"354,-0.5 354,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"421\" y=\"-31.3\">(None, None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"354,-23.5 488,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"421\" y=\"-8.3\">(None, None, 10000)</text>\n",
       "</g>\n",
       "<!-- 2750321578952&#45;&gt;2750321579568 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>2750321578952-&gt;2750321579568</title>\n",
       "<path d=\"M341,-83.3664C341,-75.1516 341,-65.6579 341,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"344.5,-56.6068 341,-46.6068 337.5,-56.6069 344.5,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualise model as a graph\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import pydot_ng as pydot\n",
    "import graphviz as graphviz\n",
    "SVG(model_to_dot(model,show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss function since sparse does not work: https://github.com/tensorflow/tensorflow/issues/17150\n",
    "def sparse_cross_entropy(y_true, y_pred):\n",
    "    # Calculate the loss. This outputs a 2-rank tensor of shape [batch_size, sequence_length]\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "    loss_mean = tf.reduce_mean(loss)\n",
    "    return loss_mean"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "optimizerxx = RMSprop(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_target = tf.placeholder(dtype='int32', shape=(None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss=sparse_cross_entropy,\n",
    "              target_tensors=[decoder_target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11871 samples, validate on 2968 samples\n",
      "Epoch 1/200\n",
      "11871/11871 [==============================] - 4s 346us/step - loss: 0.0631 - val_loss: 2.1914\n",
      "Epoch 2/200\n",
      "11871/11871 [==============================] - 4s 340us/step - loss: 0.0629 - val_loss: 2.2076\n",
      "Epoch 3/200\n",
      "11871/11871 [==============================] - 4s 339us/step - loss: 0.0617 - val_loss: 2.1970\n",
      "Epoch 4/200\n",
      "11871/11871 [==============================] - 4s 344us/step - loss: 0.0609 - val_loss: 2.2078\n",
      "Epoch 5/200\n",
      "11871/11871 [==============================] - 4s 344us/step - loss: 0.0608 - val_loss: 2.2017\n",
      "Epoch 6/200\n",
      "11871/11871 [==============================] - 4s 344us/step - loss: 0.0602 - val_loss: 2.2034\n",
      "Epoch 7/200\n",
      "11871/11871 [==============================] - 4s 340us/step - loss: 0.0596 - val_loss: 2.1967\n",
      "Epoch 8/200\n",
      "11871/11871 [==============================] - 4s 339us/step - loss: 0.0592 - val_loss: 2.2286\n",
      "Epoch 9/200\n",
      "11871/11871 [==============================] - 4s 343us/step - loss: 0.0591 - val_loss: 2.2111\n",
      "Epoch 10/200\n",
      "11871/11871 [==============================] - 4s 343us/step - loss: 0.0576 - val_loss: 2.2190\n",
      "Epoch 11/200\n",
      "11871/11871 [==============================] - 4s 349us/step - loss: 0.0574 - val_loss: 2.2241\n",
      "Epoch 12/200\n",
      "11871/11871 [==============================] - 4s 341us/step - loss: 0.0575 - val_loss: 2.2240\n",
      "Epoch 13/200\n",
      "11871/11871 [==============================] - 4s 349us/step - loss: 0.0563 - val_loss: 2.2121\n",
      "Epoch 14/200\n",
      "11871/11871 [==============================] - 4s 342us/step - loss: 0.0560 - val_loss: 2.2290\n",
      "Epoch 15/200\n",
      "11871/11871 [==============================] - 4s 340us/step - loss: 0.0554 - val_loss: 2.2372\n",
      "Epoch 16/200\n",
      "11871/11871 [==============================] - 4s 339us/step - loss: 0.0548 - val_loss: 2.2285\n",
      "Epoch 17/200\n",
      "11871/11871 [==============================] - 4s 342us/step - loss: 0.0550 - val_loss: 2.2502\n",
      "Epoch 18/200\n",
      "11871/11871 [==============================] - 4s 347us/step - loss: 0.0542 - val_loss: 2.2452\n",
      "Epoch 19/200\n",
      "11871/11871 [==============================] - 4s 342us/step - loss: 0.0545 - val_loss: 2.2360\n",
      "Epoch 20/200\n",
      "11871/11871 [==============================] - 4s 344us/step - loss: 0.0532 - val_loss: 2.2495\n",
      "Epoch 21/200\n",
      "11871/11871 [==============================] - 4s 341us/step - loss: 0.0535 - val_loss: 2.2534\n",
      "Epoch 22/200\n",
      "11871/11871 [==============================] - 4s 345us/step - loss: 0.0522 - val_loss: 2.2562\n",
      "Epoch 23/200\n",
      "11871/11871 [==============================] - 4s 343us/step - loss: 0.0519 - val_loss: 2.2585\n",
      "Epoch 24/200\n",
      "11871/11871 [==============================] - 4s 340us/step - loss: 0.0516 - val_loss: 2.2396\n",
      "Epoch 25/200\n",
      "11871/11871 [==============================] - 4s 343us/step - loss: 0.0510 - val_loss: 2.2564\n",
      "Epoch 26/200\n",
      "11871/11871 [==============================] - 4s 339us/step - loss: 0.0506 - val_loss: 2.2566\n",
      "Epoch 27/200\n",
      "11871/11871 [==============================] - 4s 343us/step - loss: 0.0504 - val_loss: 2.2722\n",
      "Epoch 28/200\n",
      "11871/11871 [==============================] - 4s 345us/step - loss: 0.0502 - val_loss: 2.2618\n",
      "Epoch 29/200\n",
      "11871/11871 [==============================] - 4s 340us/step - loss: 0.0501 - val_loss: 2.2616\n",
      "Epoch 30/200\n",
      "11871/11871 [==============================] - 4s 344us/step - loss: 0.0495 - val_loss: 2.2645\n",
      "Epoch 31/200\n",
      "11871/11871 [==============================] - 4s 343us/step - loss: 0.0489 - val_loss: 2.2615\n",
      "Epoch 32/200\n",
      "11871/11871 [==============================] - 4s 342us/step - loss: 0.0489 - val_loss: 2.2472\n",
      "Epoch 33/200\n",
      "11871/11871 [==============================] - 4s 346us/step - loss: 0.0484 - val_loss: 2.2802\n",
      "Epoch 34/200\n",
      "11871/11871 [==============================] - 4s 346us/step - loss: 0.0480 - val_loss: 2.2655\n",
      "Epoch 35/200\n",
      "11871/11871 [==============================] - 4s 343us/step - loss: 0.0478 - val_loss: 2.2739\n",
      "Epoch 36/200\n",
      "11871/11871 [==============================] - 4s 341us/step - loss: 0.0476 - val_loss: 2.2652\n",
      "Epoch 37/200\n",
      "11871/11871 [==============================] - 4s 341us/step - loss: 0.0470 - val_loss: 2.2968\n",
      "Epoch 38/200\n",
      "11871/11871 [==============================] - 4s 347us/step - loss: 0.0465 - val_loss: 2.2793\n",
      "Epoch 39/200\n",
      "11871/11871 [==============================] - 4s 341us/step - loss: 0.0466 - val_loss: 2.2822\n",
      "Epoch 40/200\n",
      "11871/11871 [==============================] - 4s 343us/step - loss: 0.0462 - val_loss: 2.3068\n",
      "Epoch 41/200\n",
      "11871/11871 [==============================] - 4s 344us/step - loss: 0.0458 - val_loss: 2.2875\n",
      "Epoch 42/200\n",
      "11871/11871 [==============================] - 4s 344us/step - loss: 0.0454 - val_loss: 2.2909\n",
      "Epoch 43/200\n",
      "11871/11871 [==============================] - 4s 344us/step - loss: 0.0447 - val_loss: 2.2789\n",
      "Epoch 44/200\n",
      "11871/11871 [==============================] - 4s 344us/step - loss: 0.0441 - val_loss: 2.2858\n",
      "Epoch 45/200\n",
      "11871/11871 [==============================] - 4s 342us/step - loss: 0.0447 - val_loss: 2.2734\n",
      "Epoch 46/200\n",
      "11871/11871 [==============================] - 4s 343us/step - loss: 0.0440 - val_loss: 2.2979\n",
      "Epoch 47/200\n",
      "11871/11871 [==============================] - 4s 341us/step - loss: 0.0441 - val_loss: 2.3002\n",
      "Epoch 48/200\n",
      "11871/11871 [==============================] - 4s 341us/step - loss: 0.0437 - val_loss: 2.2935\n",
      "Epoch 49/200\n",
      "11871/11871 [==============================] - 4s 340us/step - loss: 0.0434 - val_loss: 2.2859\n",
      "Epoch 50/200\n",
      "11871/11871 [==============================] - 4s 346us/step - loss: 0.0429 - val_loss: 2.3019\n",
      "Epoch 51/200\n",
      "11871/11871 [==============================] - 4s 342us/step - loss: 0.0429 - val_loss: 2.2881\n",
      "Epoch 52/200\n",
      "11871/11871 [==============================] - 4s 346us/step - loss: 0.0422 - val_loss: 2.2862\n",
      "Epoch 53/200\n",
      "11871/11871 [==============================] - 4s 341us/step - loss: 0.0422 - val_loss: 2.2941\n",
      "Epoch 54/200\n",
      "11871/11871 [==============================] - 4s 341us/step - loss: 0.0425 - val_loss: 2.2983\n",
      "Epoch 55/200\n",
      "11871/11871 [==============================] - 4s 345us/step - loss: 0.0413 - val_loss: 2.3027\n",
      "Epoch 56/200\n",
      "11871/11871 [==============================] - 4s 341us/step - loss: 0.0414 - val_loss: 2.3085\n",
      "Epoch 57/200\n",
      "11871/11871 [==============================] - 4s 349us/step - loss: 0.0412 - val_loss: 2.3055\n",
      "Epoch 58/200\n",
      "11871/11871 [==============================] - 4s 346us/step - loss: 0.0409 - val_loss: 2.2855\n",
      "Epoch 59/200\n",
      "11871/11871 [==============================] - 4s 341us/step - loss: 0.0408 - val_loss: 2.3091\n",
      "Epoch 60/200\n",
      "11871/11871 [==============================] - 4s 345us/step - loss: 0.0408 - val_loss: 2.3146\n",
      "Epoch 61/200\n",
      "11871/11871 [==============================] - 4s 345us/step - loss: 0.0402 - val_loss: 2.3160\n",
      "Epoch 62/200\n",
      "11871/11871 [==============================] - 4s 339us/step - loss: 0.0404 - val_loss: 2.3127\n",
      "Epoch 63/200\n",
      "11871/11871 [==============================] - 4s 345us/step - loss: 0.0402 - val_loss: 2.3059\n",
      "Epoch 64/200\n",
      "11871/11871 [==============================] - 4s 345us/step - loss: 0.0397 - val_loss: 2.3051\n",
      "Epoch 65/200\n",
      "11871/11871 [==============================] - 4s 342us/step - loss: 0.0399 - val_loss: 2.3058\n",
      "Epoch 66/200\n",
      "11871/11871 [==============================] - 4s 347us/step - loss: 0.0392 - val_loss: 2.3137\n",
      "Epoch 67/200\n",
      "11871/11871 [==============================] - 4s 340us/step - loss: 0.0394 - val_loss: 2.3045\n",
      "Epoch 68/200\n",
      "11871/11871 [==============================] - 4s 343us/step - loss: 0.0384 - val_loss: 2.3193\n",
      "Epoch 69/200\n",
      "11871/11871 [==============================] - 4s 343us/step - loss: 0.0382 - val_loss: 2.3349\n",
      "Epoch 70/200\n",
      "11871/11871 [==============================] - 4s 346us/step - loss: 0.0387 - val_loss: 2.3177\n",
      "Epoch 71/200\n",
      "11871/11871 [==============================] - 4s 345us/step - loss: 0.0381 - val_loss: 2.3400\n",
      "Epoch 72/200\n",
      "11871/11871 [==============================] - 4s 348us/step - loss: 0.0385 - val_loss: 2.3336\n",
      "Epoch 73/200\n",
      "11871/11871 [==============================] - 4s 344us/step - loss: 0.0380 - val_loss: 2.3398\n",
      "Epoch 74/200\n",
      "11871/11871 [==============================] - 4s 342us/step - loss: 0.0378 - val_loss: 2.3152\n",
      "Epoch 75/200\n",
      "11871/11871 [==============================] - 4s 347us/step - loss: 0.0371 - val_loss: 2.3335\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11871/11871 [==============================] - 4s 345us/step - loss: 0.0366 - val_loss: 2.3387\n",
      "Epoch 77/200\n",
      "11871/11871 [==============================] - 4s 341us/step - loss: 0.0368 - val_loss: 2.3356\n",
      "Epoch 78/200\n",
      "11871/11871 [==============================] - 4s 340us/step - loss: 0.0363 - val_loss: 2.3521\n",
      "Epoch 79/200\n",
      "11871/11871 [==============================] - 4s 347us/step - loss: 0.0367 - val_loss: 2.3523\n",
      "Epoch 80/200\n",
      "11871/11871 [==============================] - 4s 342us/step - loss: 0.0366 - val_loss: 2.3377\n",
      "Epoch 81/200\n",
      "11871/11871 [==============================] - 4s 343us/step - loss: 0.0364 - val_loss: 2.3571\n",
      "Epoch 82/200\n",
      "11871/11871 [==============================] - 4s 346us/step - loss: 0.0357 - val_loss: 2.3346\n",
      "Epoch 83/200\n",
      "11871/11871 [==============================] - 4s 343us/step - loss: 0.0361 - val_loss: 2.3386\n",
      "Epoch 84/200\n",
      "11871/11871 [==============================] - 4s 354us/step - loss: 0.0358 - val_loss: 2.3528\n",
      "Epoch 85/200\n",
      "11871/11871 [==============================] - 4s 352us/step - loss: 0.0359 - val_loss: 2.3477\n",
      "Epoch 86/200\n",
      "11871/11871 [==============================] - 4s 356us/step - loss: 0.0357 - val_loss: 2.3437\n",
      "Epoch 87/200\n",
      "11871/11871 [==============================] - 4s 348us/step - loss: 0.0352 - val_loss: 2.3474\n",
      "Epoch 88/200\n",
      "11871/11871 [==============================] - 4s 344us/step - loss: 0.0350 - val_loss: 2.3349\n",
      "Epoch 89/200\n",
      "11871/11871 [==============================] - 4s 344us/step - loss: 0.0349 - val_loss: 2.3546\n",
      "Epoch 90/200\n",
      "11871/11871 [==============================] - 4s 342us/step - loss: 0.0342 - val_loss: 2.3731\n",
      "Epoch 91/200\n",
      "11871/11871 [==============================] - 4s 349us/step - loss: 0.0346 - val_loss: 2.3469\n",
      "Epoch 92/200\n",
      "11871/11871 [==============================] - 4s 356us/step - loss: 0.0345 - val_loss: 2.3611\n",
      "Epoch 93/200\n",
      "11871/11871 [==============================] - 4s 347us/step - loss: 0.0336 - val_loss: 2.3611\n",
      "Epoch 94/200\n",
      "11871/11871 [==============================] - 4s 345us/step - loss: 0.0344 - val_loss: 2.3510\n",
      "Epoch 95/200\n",
      "11871/11871 [==============================] - 4s 343us/step - loss: 0.0342 - val_loss: 2.3644\n",
      "Epoch 96/200\n",
      "11871/11871 [==============================] - 4s 349us/step - loss: 0.0336 - val_loss: 2.3676\n",
      "Epoch 97/200\n",
      "11871/11871 [==============================] - 4s 341us/step - loss: 0.0337 - val_loss: 2.3676\n",
      "Epoch 98/200\n",
      "11871/11871 [==============================] - 4s 353us/step - loss: 0.0338 - val_loss: 2.3584\n",
      "Epoch 99/200\n",
      "11871/11871 [==============================] - 4s 350us/step - loss: 0.0334 - val_loss: 2.3656\n",
      "Epoch 100/200\n",
      "11871/11871 [==============================] - 4s 372us/step - loss: 0.0333 - val_loss: 2.3689\n",
      "Epoch 101/200\n",
      "11871/11871 [==============================] - 4s 348us/step - loss: 0.0329 - val_loss: 2.3788\n",
      "Epoch 102/200\n",
      "11871/11871 [==============================] - 4s 350us/step - loss: 0.0331 - val_loss: 2.3783\n",
      "Epoch 103/200\n",
      "11871/11871 [==============================] - 4s 347us/step - loss: 0.0325 - val_loss: 2.3650\n",
      "Epoch 104/200\n",
      "11871/11871 [==============================] - 4s 345us/step - loss: 0.0327 - val_loss: 2.3624\n",
      "Epoch 105/200\n",
      "11871/11871 [==============================] - 4s 353us/step - loss: 0.0328 - val_loss: 2.3671\n",
      "Epoch 106/200\n",
      "11871/11871 [==============================] - 4s 365us/step - loss: 0.0321 - val_loss: 2.3794\n",
      "Epoch 107/200\n",
      "11871/11871 [==============================] - 4s 356us/step - loss: 0.0322 - val_loss: 2.3811\n",
      "Epoch 108/200\n",
      "11871/11871 [==============================] - 4s 369us/step - loss: 0.0322 - val_loss: 2.3845\n",
      "Epoch 109/200\n",
      "11871/11871 [==============================] - 4s 354us/step - loss: 0.0320 - val_loss: 2.3838\n",
      "Epoch 110/200\n",
      "11871/11871 [==============================] - 4s 362us/step - loss: 0.0319 - val_loss: 2.3661\n",
      "Epoch 111/200\n",
      "11871/11871 [==============================] - 4s 354us/step - loss: 0.0314 - val_loss: 2.3926\n",
      "Epoch 112/200\n",
      "11871/11871 [==============================] - 4s 365us/step - loss: 0.0316 - val_loss: 2.3849\n",
      "Epoch 113/200\n",
      "11871/11871 [==============================] - 4s 351us/step - loss: 0.0315 - val_loss: 2.3936\n",
      "Epoch 114/200\n",
      "11871/11871 [==============================] - 4s 360us/step - loss: 0.0314 - val_loss: 2.3929\n",
      "Epoch 115/200\n",
      "11871/11871 [==============================] - 4s 358us/step - loss: 0.0313 - val_loss: 2.3953\n",
      "Epoch 116/200\n",
      "11871/11871 [==============================] - 4s 357us/step - loss: 0.0311 - val_loss: 2.3862\n",
      "Epoch 117/200\n",
      "11871/11871 [==============================] - 4s 368us/step - loss: 0.0311 - val_loss: 2.3924\n",
      "Epoch 118/200\n",
      "11871/11871 [==============================] - 4s 363us/step - loss: 0.0305 - val_loss: 2.3849\n",
      "Epoch 119/200\n",
      "11871/11871 [==============================] - 4s 358us/step - loss: 0.0308 - val_loss: 2.3874\n",
      "Epoch 120/200\n",
      "11871/11871 [==============================] - 4s 364us/step - loss: 0.0308 - val_loss: 2.3892\n",
      "Epoch 121/200\n",
      "11871/11871 [==============================] - 4s 367us/step - loss: 0.0308 - val_loss: 2.3917\n",
      "Epoch 122/200\n",
      "11871/11871 [==============================] - 4s 364us/step - loss: 0.0305 - val_loss: 2.3894\n",
      "Epoch 123/200\n",
      "11871/11871 [==============================] - 4s 361us/step - loss: 0.0305 - val_loss: 2.3967\n",
      "Epoch 124/200\n",
      "11871/11871 [==============================] - 4s 365us/step - loss: 0.0304 - val_loss: 2.4148\n",
      "Epoch 125/200\n",
      "11871/11871 [==============================] - 4s 370us/step - loss: 0.0303 - val_loss: 2.3775\n",
      "Epoch 126/200\n",
      "11871/11871 [==============================] - 4s 363us/step - loss: 0.0299 - val_loss: 2.3936\n",
      "Epoch 127/200\n",
      "11871/11871 [==============================] - 4s 360us/step - loss: 0.0297 - val_loss: 2.3993\n",
      "Epoch 128/200\n",
      "11871/11871 [==============================] - 4s 368us/step - loss: 0.0300 - val_loss: 2.4028\n",
      "Epoch 129/200\n",
      "11871/11871 [==============================] - 4s 365us/step - loss: 0.0297 - val_loss: 2.3990\n",
      "Epoch 130/200\n",
      "11871/11871 [==============================] - 4s 365us/step - loss: 0.0295 - val_loss: 2.4162\n",
      "Epoch 131/200\n",
      "11871/11871 [==============================] - 4s 367us/step - loss: 0.0297 - val_loss: 2.4110\n",
      "Epoch 132/200\n",
      "11871/11871 [==============================] - 4s 362us/step - loss: 0.0289 - val_loss: 2.4095\n",
      "Epoch 133/200\n",
      "11871/11871 [==============================] - 4s 360us/step - loss: 0.0293 - val_loss: 2.4143\n",
      "Epoch 134/200\n",
      "11871/11871 [==============================] - 4s 365us/step - loss: 0.0294 - val_loss: 2.3902\n",
      "Epoch 135/200\n",
      "11871/11871 [==============================] - 4s 365us/step - loss: 0.0294 - val_loss: 2.4129\n",
      "Epoch 136/200\n",
      "11871/11871 [==============================] - 4s 373us/step - loss: 0.0290 - val_loss: 2.4077\n",
      "Epoch 137/200\n",
      "11871/11871 [==============================] - 4s 367us/step - loss: 0.0289 - val_loss: 2.4011\n",
      "Epoch 138/200\n",
      "11871/11871 [==============================] - 4s 364us/step - loss: 0.0291 - val_loss: 2.3895\n",
      "Epoch 139/200\n",
      "11871/11871 [==============================] - 4s 365us/step - loss: 0.0287 - val_loss: 2.4034\n",
      "Epoch 140/200\n",
      "11871/11871 [==============================] - 4s 370us/step - loss: 0.0283 - val_loss: 2.4121\n",
      "Epoch 141/200\n",
      "11871/11871 [==============================] - 4s 363us/step - loss: 0.0286 - val_loss: 2.4118\n",
      "Epoch 142/200\n",
      "11871/11871 [==============================] - 4s 365us/step - loss: 0.0288 - val_loss: 2.4218\n",
      "Epoch 143/200\n",
      "11871/11871 [==============================] - 4s 367us/step - loss: 0.0283 - val_loss: 2.4333\n",
      "Epoch 144/200\n",
      "11871/11871 [==============================] - 4s 360us/step - loss: 0.0283 - val_loss: 2.4177\n",
      "Epoch 145/200\n",
      "11871/11871 [==============================] - 4s 358us/step - loss: 0.0288 - val_loss: 2.4308\n",
      "Epoch 146/200\n",
      "11871/11871 [==============================] - 4s 360us/step - loss: 0.0282 - val_loss: 2.4206\n",
      "Epoch 147/200\n",
      "11871/11871 [==============================] - 4s 356us/step - loss: 0.0282 - val_loss: 2.4299\n",
      "Epoch 148/200\n",
      "11871/11871 [==============================] - 4s 351us/step - loss: 0.0276 - val_loss: 2.4215\n",
      "Epoch 149/200\n",
      "11871/11871 [==============================] - 4s 363us/step - loss: 0.0279 - val_loss: 2.4255\n",
      "Epoch 150/200\n",
      "11871/11871 [==============================] - 4s 362us/step - loss: 0.0275 - val_loss: 2.4193\n",
      "Epoch 151/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11871/11871 [==============================] - 4s 350us/step - loss: 0.0271 - val_loss: 2.4427\n",
      "Epoch 152/200\n",
      "11871/11871 [==============================] - 4s 355us/step - loss: 0.0281 - val_loss: 2.4348\n",
      "Epoch 153/200\n",
      "11871/11871 [==============================] - 4s 356us/step - loss: 0.0274 - val_loss: 2.4196\n",
      "Epoch 154/200\n",
      "11871/11871 [==============================] - 4s 354us/step - loss: 0.0275 - val_loss: 2.4393\n",
      "Epoch 155/200\n",
      "11871/11871 [==============================] - 4s 362us/step - loss: 0.0272 - val_loss: 2.4319\n",
      "Epoch 156/200\n",
      "11871/11871 [==============================] - 4s 360us/step - loss: 0.0273 - val_loss: 2.4361\n",
      "Epoch 157/200\n",
      "11871/11871 [==============================] - 4s 360us/step - loss: 0.0276 - val_loss: 2.4263\n",
      "Epoch 158/200\n",
      "11871/11871 [==============================] - 4s 358us/step - loss: 0.0267 - val_loss: 2.4316\n",
      "Epoch 159/200\n",
      "11871/11871 [==============================] - 4s 359us/step - loss: 0.0270 - val_loss: 2.4392\n",
      "Epoch 160/200\n",
      "11871/11871 [==============================] - 4s 351us/step - loss: 0.0268 - val_loss: 2.4298\n",
      "Epoch 161/200\n",
      "11871/11871 [==============================] - 4s 353us/step - loss: 0.0269 - val_loss: 2.4528\n",
      "Epoch 162/200\n",
      "11871/11871 [==============================] - 4s 355us/step - loss: 0.0266 - val_loss: 2.4451\n",
      "Epoch 163/200\n",
      "11871/11871 [==============================] - 4s 363us/step - loss: 0.0270 - val_loss: 2.4554\n",
      "Epoch 164/200\n",
      "11871/11871 [==============================] - 4s 354us/step - loss: 0.0266 - val_loss: 2.4457\n",
      "Epoch 165/200\n",
      "11871/11871 [==============================] - 4s 356us/step - loss: 0.0268 - val_loss: 2.4277\n",
      "Epoch 166/200\n",
      "11871/11871 [==============================] - 4s 355us/step - loss: 0.0264 - val_loss: 2.4720\n",
      "Epoch 167/200\n",
      "11871/11871 [==============================] - 4s 357us/step - loss: 0.0269 - val_loss: 2.4375\n",
      "Epoch 168/200\n",
      "11871/11871 [==============================] - 4s 360us/step - loss: 0.0267 - val_loss: 2.4605\n",
      "Epoch 169/200\n",
      "11871/11871 [==============================] - 4s 355us/step - loss: 0.0264 - val_loss: 2.4537\n",
      "Epoch 170/200\n",
      "11871/11871 [==============================] - 4s 355us/step - loss: 0.0260 - val_loss: 2.4408\n",
      "Epoch 171/200\n",
      "11871/11871 [==============================] - 4s 353us/step - loss: 0.0261 - val_loss: 2.4528\n",
      "Epoch 172/200\n",
      "11871/11871 [==============================] - 4s 353us/step - loss: 0.0263 - val_loss: 2.4540\n",
      "Epoch 173/200\n",
      "11871/11871 [==============================] - 4s 344us/step - loss: 0.0260 - val_loss: 2.4602\n",
      "Epoch 174/200\n",
      "11871/11871 [==============================] - 4s 344us/step - loss: 0.0260 - val_loss: 2.4577\n",
      "Epoch 175/200\n",
      "11871/11871 [==============================] - 4s 343us/step - loss: 0.0260 - val_loss: 2.4651\n",
      "Epoch 176/200\n",
      "11871/11871 [==============================] - 4s 345us/step - loss: 0.0258 - val_loss: 2.4649\n",
      "Epoch 177/200\n",
      "11871/11871 [==============================] - 4s 358us/step - loss: 0.0255 - val_loss: 2.4634\n",
      "Epoch 178/200\n",
      "11871/11871 [==============================] - 4s 363us/step - loss: 0.0259 - val_loss: 2.4469\n",
      "Epoch 179/200\n",
      "11871/11871 [==============================] - 4s 353us/step - loss: 0.0255 - val_loss: 2.4569\n",
      "Epoch 180/200\n",
      "11871/11871 [==============================] - 4s 351us/step - loss: 0.0255 - val_loss: 2.4658\n",
      "Epoch 181/200\n",
      "11871/11871 [==============================] - 4s 367us/step - loss: 0.0254 - val_loss: 2.4734\n",
      "Epoch 182/200\n",
      "11871/11871 [==============================] - 4s 360us/step - loss: 0.0251 - val_loss: 2.4685\n",
      "Epoch 183/200\n",
      "11871/11871 [==============================] - 4s 344us/step - loss: 0.0253 - val_loss: 2.4592\n",
      "Epoch 184/200\n",
      "11871/11871 [==============================] - 4s 360us/step - loss: 0.0255 - val_loss: 2.4600\n",
      "Epoch 185/200\n",
      "11871/11871 [==============================] - 4s 355us/step - loss: 0.0252 - val_loss: 2.4679\n",
      "Epoch 186/200\n",
      "11871/11871 [==============================] - 4s 357us/step - loss: 0.0250 - val_loss: 2.4678\n",
      "Epoch 187/200\n",
      "11871/11871 [==============================] - 4s 360us/step - loss: 0.0253 - val_loss: 2.4772\n",
      "Epoch 188/200\n",
      "11871/11871 [==============================] - 4s 357us/step - loss: 0.0252 - val_loss: 2.4791\n",
      "Epoch 189/200\n",
      "11871/11871 [==============================] - 4s 363us/step - loss: 0.0248 - val_loss: 2.4701\n",
      "Epoch 190/200\n",
      "11871/11871 [==============================] - 4s 355us/step - loss: 0.0249 - val_loss: 2.4778\n",
      "Epoch 191/200\n",
      "11871/11871 [==============================] - 4s 361us/step - loss: 0.0250 - val_loss: 2.4740\n",
      "Epoch 192/200\n",
      "11871/11871 [==============================] - 4s 362us/step - loss: 0.0253 - val_loss: 2.4601\n",
      "Epoch 193/200\n",
      "11871/11871 [==============================] - 4s 360us/step - loss: 0.0247 - val_loss: 2.4729\n",
      "Epoch 194/200\n",
      "11871/11871 [==============================] - 4s 360us/step - loss: 0.0247 - val_loss: 2.4605\n",
      "Epoch 195/200\n",
      "11871/11871 [==============================] - 4s 360us/step - loss: 0.0243 - val_loss: 2.4779\n",
      "Epoch 196/200\n",
      "11871/11871 [==============================] - 4s 359us/step - loss: 0.0247 - val_loss: 2.4790\n",
      "Epoch 197/200\n",
      "11871/11871 [==============================] - 4s 358us/step - loss: 0.0246 - val_loss: 2.4762\n",
      "Epoch 198/200\n",
      "11871/11871 [==============================] - 4s 366us/step - loss: 0.0247 - val_loss: 2.4924\n",
      "Epoch 199/200\n",
      "11871/11871 [==============================] - 4s 361us/step - loss: 0.0245 - val_loss: 2.4489\n",
      "Epoch 200/200\n",
      "11871/11871 [==============================] - 4s 365us/step - loss: 0.0244 - val_loss: 2.4621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ThomasGordon\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py:2368: UserWarning: Layer gru_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'gru_1/while/Exit_2:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# Note that `decoder_target_data` needs to be one-hot encoded,\n",
    "# rather than sequences of integers like `decoder_input_data`!\n",
    "history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,                # _onehot\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=numEpochs,\n",
    "                    validation_split=0.2)\n",
    "model.save('TGC_trans.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'loss'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x281ea52ad68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotter historikken for 'loss'\n",
    "loss     = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, numEpochs+1)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')       # bo = \"blue dot\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')  # b  = \"solid blue line\"\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference mode = testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create sampling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, None, 256)         2560000   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  [(None, 256), (None, 256) 393984    \n",
      "=================================================================\n",
      "Total params: 2,953,984\n",
      "Trainable params: 2,953,984\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# encoder model used to create internal representation / states\n",
    "encoder_model = Model(encoder_inputs, state_h)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 256)    2560000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     [(None, None, 256),  393984      embedding_2[1][0]                \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 10000)  2570000     gru_2[1][0]                      \n",
      "==================================================================================================\n",
      "Total params: 5,523,984\n",
      "Trainable params: 5,523,984\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_inputs = [decoder_state_input_h,]\n",
    "\n",
    "# reuse the decoder we have trained\n",
    "decoder_embed_final2 = decoder_embed(decoder_inputs)\n",
    "\n",
    "decoder_outputs2, state_h2 = decoder_gru(decoder_embed_final2, initial_state=decoder_state_inputs)\n",
    "\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_state_inputs, \n",
    "    [decoder_outputs2] + [state_h2])                    # notice the '+' operator requires [] to work !!!\n",
    "\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "   \n",
    "    # tokenize the text to be translated, and reverse\n",
    "    input_tokens = tokenizer_inp.texts_to_sequences([input_seq])\n",
    "    input_tokens = pad_sequences(input_tokens,\n",
    "                                 maxlen=max_tokens_input,\n",
    "                                 padding='post',\n",
    "                                 truncating='post')\n",
    "\n",
    "    # encode the input sentence\n",
    "    states_value = encoder_model.predict(input_tokens)\n",
    "    \n",
    "    # Generate empty target sequence of length 1 and insert start token\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = token_start    #\n",
    "\n",
    "    # sampling loop to generate translated words using decoder, word by word\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    while not stop_condition:  \n",
    "        # predict one next word, decoder returns probabilities for all words/tokens\n",
    "        output_tokens, h = decoder_model.predict([target_seq] + [states_value])\n",
    "        \n",
    "        # pick most probable token / word\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = index_to_word_target[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_word\n",
    "        \n",
    "        # Exit condition: either hit max length or find stop character.\n",
    "        if (sampled_word == 'eeee' or len(decoded_sentence) > 52):\n",
    "            stop_condition = True\n",
    "        \n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        \n",
    "        # Update states, so they can be re-injected in next token/word prediction\n",
    "        states_value = h\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing translation ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can't both be right.  i kan ikke begge have ret eeee \n",
      "\n",
      "You can't get rid of me.  du kan ikke slippe af med mig eeee \n",
      "\n",
      "You can't prove a thing.  du kan ikke bevise noget eeee \n",
      "\n",
      "You do not have a fever.  du har ikke feber eeee \n",
      "\n",
      "You don't know who I am.  du ved ikke hvem jeg er eeee \n",
      "\n",
      "You don't need to panic.  du behøver ikke at gå i panik eeee \n",
      "\n",
      "You have very sexy legs.  du har meget sexede ben eeee \n",
      "\n",
      "You left your lights on.  du har lys på eeee \n",
      "\n",
      "You left your lights on.  du har lys på eeee \n",
      "\n",
      "You must've been asleep.  du må have sovet eeee \n",
      "\n",
      "You need an interpreter.  du har brug for en tolk eeee \n",
      "\n",
      "You never stop learning.  man skal lære så længe man lever eeee \n",
      "\n",
      "You should quit smoking.  du burde holde op med at ryge eeee \n",
      "\n",
      "You should quit smoking.  du burde holde op med at ryge eeee \n",
      "\n",
      "You surprised everybody.  du overraskede alle eeee \n",
      "\n",
      "You won't have a choice.  du får ikke et valg eeee \n",
      "\n",
      "You're a beautiful girl.  du er en smuk pige eeee \n",
      "\n",
      "You're a very lucky man.  du er en meget heldig mand eeee \n",
      "\n",
      "You're absolutely right.  du har fuldstændig ret eeee \n",
      "\n",
      "You're completely right.  du har fuldstændig ret eeee \n",
      "\n",
      "You're driving too fast.  du kører for hurtigt eeee \n",
      "\n",
      "You're the problem, Tom.  du er problemet tom eeee \n",
      "\n",
      "You're white as a sheet.  du er hvid som et lagen eeee \n",
      "\n",
      "Your brother needs help.  din bror har brug for hjælp eeee \n",
      "\n",
      "Your guests are waiting.  dine gæster venter allerede eeee \n",
      "\n",
      "Your guests are waiting.  dine gæster venter allerede eeee \n",
      "\n",
      "Your house is fantastic.  dit hus er fantastisk eeee \n",
      "\n",
      "Your name is Tom, right?  dit navn er tom ikke sandt eeee \n",
      "\n",
      "A lot of kids wear jeans.  mange børn går med jeans eeee \n",
      "\n",
      "A lot of kids wear jeans.  mange børn går med jeans eeee \n",
      "\n",
      "A lot of kids wear jeans.  mange børn går med jeans eeee \n",
      "\n",
      "A lot of kids wear jeans.  mange børn går med jeans eeee \n",
      "\n",
      "A lot of kids wear jeans.  mange børn går med jeans eeee \n",
      "\n",
      "A tea with lemon, please.  en te med citron tak eeee \n",
      "\n",
      "A year has twelve months.  et år har tolv måneder eeee \n",
      "\n",
      "All Tom does is watch TV.  tom laver ikke andet end at se fjernsyn eeee \n",
      "\n",
      "All the apples are there.  alle æblerne er der eeee \n",
      "\n",
      "Am I disturbing anything?  forstyrrer jeg eeee \n",
      "\n",
      "An accident has happened.  der er sket en ulykke eeee \n",
      "\n",
      "Antibiotics are overused.  der er et overforbrug af antibiotika eeee \n",
      "\n",
      "Are all the doors locked?  er alle døre låst eeee \n",
      "\n",
      "Are they speaking French?  taler de fransk eeee \n",
      "\n",
      "Are they speaking French?  taler de fransk eeee \n",
      "\n",
      "Are you able to solve it?  er du i stand til at løse det eeee \n",
      "\n",
      "Are you able to solve it?  er du i stand til at løse det eeee \n",
      "\n",
      "Are you going to kiss me?  kysser du mig eeee \n",
      "\n",
      "Are you here with anyone?  er du her sammen med nogen eeee \n",
      "\n",
      "Are you in love with Tom?  er du forelsket i tom eeee \n",
      "\n",
      "Are you wearing lipstick?  har du læbestift på eeee \n",
      "\n",
      "Ask me anything you want.  spørg mig om alt hvad du vil eeee \n",
      "\n",
      "Barking dogs seldom bite.  hunde der gør bider sjældent eeee \n",
      "\n",
      "Barking dogs seldom bite.  hunde der gør bider sjældent eeee \n",
      "\n",
      "Butter is made from milk.  smør laver man af mælk eeee \n",
      "\n",
      "Butter is made from milk.  smør laver man af mælk eeee \n",
      "\n",
      "Can I borrow your pencil?  må jeg låne din blyant eeee \n",
      "\n",
      "Can I count on your help?  kan jeg regne med din hjælp eeee \n",
      "\n",
      "Can I get your autograph?  kan jeg få din autograf eeee \n",
      "\n",
      "Can I use your telephone?  må jeg bruge din telefon eeee \n",
      "\n",
      "Can I use your telephone?  må jeg bruge din telefon eeee \n",
      "\n",
      "Canada is not a paradise.  canada er ikke et paradis eeee \n",
      "\n",
      "Cats can see in the dark.  katte kan se i mørke eeee \n",
      "\n",
      "Christmas is a week away.  der er en uge til jul eeee \n",
      "\n",
      "Christmas is approaching.  julen nærmer sig eeee \n",
      "\n",
      "Christmas is coming soon.  det er snart jul eeee \n",
      "\n",
      "Come in, the door's open.  kom ind døren er åben eeee \n",
      "\n",
      "Could you lend me a hand?  giv mig en hånd eeee \n",
      "\n",
      "Curiosity killed the cat.  nysgerrighed dræbte katten eeee \n",
      "\n",
      "Darwin changed the world.  darwin ændrede verden eeee \n",
      "\n",
      "Did I hurt your feelings?  sårede jeg dine følelser eeee \n",
      "\n",
      "Did I hurt your feelings?  sårede jeg dine følelser eeee \n",
      "\n",
      "Did she forget her money?  glemte hun sine penge eeee \n",
      "\n",
      "Did you do your homework?  har du lavet dine lektier eeee \n",
      "\n",
      "Did you eat your spinach?  har du spist din spinat eeee \n",
      "\n",
      "Did you eat your spinach?  har du spist din spinat eeee \n",
      "\n",
      "Did you put on sunscreen?  har du smurt dig ind i solcreme eeee \n",
      "\n",
      "Did you sew this by hand?  syede du det i hånden eeee \n",
      "\n",
      "Do as I say, not as I do.  gør som jeg siger ikke som jeg gør eeee \n",
      "\n",
      "Do you believe in ghosts?  tror du på spøgelser eeee \n",
      "\n",
      "Do you guys remember Tom?  kan i huske tom eeee \n",
      "\n",
      "Do you have a dictionary?  har du en ordbog eeee \n",
      "\n",
      "Do you have a girlfriend?  har du en kæreste eeee \n",
      "\n",
      "Do you have a girlfriend?  har du en kæreste eeee \n",
      "\n",
      "Do you have a motorcycle?  har du en motorcykel eeee \n",
      "\n",
      "Do you have a smartphone?  har du en mobiltelefon eeee \n",
      "\n",
      "Do you have a smartphone?  har du en mobiltelefon eeee \n",
      "\n",
      "Do you have another plan?  har du en anden plan eeee \n",
      "\n",
      "Do you have small change?  har du småpenge eeee \n",
      "\n",
      "Do you know how to drive?  kan du køre bil eeee \n",
      "\n",
      "Do you know what love is?  ved du hvad kærlighed er eeee \n",
      "\n",
      "Do you like Chinese food?  kan du lide kinesisk mad eeee \n",
      "\n",
      "Do you like to play golf?  kan du lide at spille golf eeee \n",
      "\n",
      "Do you remember his name?  kan du huske hans navn eeee \n",
      "\n",
      "Do you still hate French?  hader du stadigvæk fransk eeee \n",
      "\n",
      "Dog is man's best friend.  hunden er menneskets bedste ven eeee \n",
      "\n",
      "Don't be late for school.  kom ikke for sent i skole eeee \n",
      "\n",
      "Don't be late for school.  kom ikke for sent i skole eeee \n",
      "\n",
      "Don't come in! I'm naked.  kom ikke ind jeg er nøgen eeee \n",
      "\n",
      "Don't forget your ticket.  glem ikke din billet eeee \n",
      "\n",
      "Don't forget your ticket.  glem ikke din billet eeee \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing on known sentences from training data\n",
    "for idx in range(6001, 6100):\n",
    "    input_seq = input_texts[idx]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(input_seq, decoded_sentence, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know and I don't want to know.  jeg ved ikke hvad jeg skal sige til det eeee \n",
      "\n",
      "I don't like learning irregular verbs.  jeg kan ikke lide at arbejde i haven eeee \n",
      "\n",
      "I don't like learning irregular verbs.  jeg kan ikke lide at arbejde i haven eeee \n",
      "\n",
      "I don't like the way she speaks to me.  jeg kan ikke lide at tale med oversygeplejersken eeee \n",
      "\n",
      "I get a call from her once in a while.  jeg har tænkt mig at kalde på en kat eeee \n",
      "\n",
      "I have a surprise for you, sweetheart.  jeg har en uge til at lave mad i dag eeee \n",
      "\n",
      "I have heard that song sung in French.  jeg har hørt hvert et ord eeee \n",
      "\n",
      "I have no problem letting others help.  jeg har ingen idé om hvad der skete eeee \n",
      "\n",
      "I heard a dog barking in the distance.  jeg har hørt om det i radioen eeee \n",
      "\n",
      "I hope that all your dreams come true.  jeg håber at det var din eneste chance eeee \n",
      "\n",
      "I know that some people value my work.  jeg ved hvor man arbejder på eeee \n",
      "\n",
      "I like May the best of all the months.  jeg kan bedre lide bananer end sort te eeee \n",
      "\n",
      "I like singing and playing the guitar.  jeg kan godt lide at tale fransk eeee \n",
      "\n",
      "I seem to have left my wallet at home.  jeg har set min pung derhjemme eeee \n",
      "\n",
      "I seem to have left my wallet at home.  jeg har set min pung derhjemme eeee \n",
      "\n",
      "I speak Japanese, English, and French.  jeg taler fransk til engelsk eeee \n",
      "\n",
      "I think that an angel watches over me.  jeg tror jeg har fået nok af det eeee \n",
      "\n",
      "I thought Tom was one of your friends.  jeg troede at tom var en af dine venner eeee \n",
      "\n",
      "I took my temperature every six hours.  jeg tog mit kamera med en kat eeee \n",
      "\n",
      "I wanted to make some telephone calls.  jeg ønskede at hyre en coach eeee \n",
      "\n",
      "I was in the bath when the phone rang.  jeg var hjemme i aften eeee \n",
      "\n",
      "I was in the bath when the phone rang.  jeg var hjemme i aften eeee \n",
      "\n",
      "I was told that I should see a doctor.  jeg gav dig alt det han ønskede eeee \n",
      "\n",
      "I would act differently in your place.  jeg ville hellere tage afsted eeee \n",
      "\n",
      "I would act differently in your place.  jeg ville hellere tage afsted eeee \n",
      "\n",
      "I would like to have a room of my own.  jeg ville gerne have et værelse med aircondition eeee \n",
      "\n",
      "I would like to see Tom one more time.  jeg vil gerne have to kilo æbler eeee \n",
      "\n",
      "I'd like to buy three of these apples.  jeg vil gerne købe et køleskab eeee \n",
      "\n",
      "I'll be away for the rest of the year.  jeg kommer tilbage om søndagen eeee \n",
      "\n",
      "I'll buy the wine and the apple juice.  jeg lå på og ventede på tom eeee \n",
      "\n",
      "I'll give you a day to think about it.  jeg vil gerne have dig til at blive fransklærer eeee \n",
      "\n",
      "I'll give you a day to think about it.  jeg vil gerne have dig til at blive fransklærer eeee \n",
      "\n",
      "I'll never forget what you did for me.  jeg gør det aldrig mere for hver dag der går eeee \n",
      "\n",
      "I'm afraid I'm not much of a musician.  jeg er bange for at blive forkølet eeee \n",
      "\n",
      "I'm listening to Björk's latest song.  jeg hører at hun ville gå ud af det her eeee \n",
      "\n",
      "I'm sorry for the delay in responding.  undskyld jeg bander på eeee \n",
      "\n",
      "I'm supposed to be here all afternoon.  jeg er kun for sent til dig eeee \n",
      "\n",
      "I've been invited to a party tomorrow.  jeg har været i boston med tre år eeee \n",
      "\n",
      "If I had enough money, I could buy it.  hvis jeg havde lidt mere end jeg for mig eeee \n",
      "\n",
      "If I were you, I'd go home right away.  hvis jeg var dig ville jeg købe noget eeee \n",
      "\n",
      "If you don't behave, Santa won't come.  hvis du ikke nok være i fred eeee \n",
      "\n",
      "If you're not quiet, they'll hear you.  du behøver ikke at komme med dig eeee \n",
      "\n",
      "In what year did the Berlin Wall fall?  om hvor tog du til frokost i dag eeee \n",
      "\n",
      "Is that why Tom wanted to talk to you?  er det det tom fortalte dig om det her eeee \n",
      "\n",
      "Is that why Tom wanted to talk to you?  er det det tom fortalte dig om det her eeee \n",
      "\n",
      "Is there any likelihood of his coming?  er der nogen chance for at tom kan vinde eeee \n",
      "\n",
      "It is colder this year than last year.  det er på høje tid at sige farvel eeee \n",
      "\n",
      "It is difficult to solve this problem.  det er vanskeligt at løse dette problem eeee \n",
      "\n",
      "It wasn't me who translated this text.  det var ikke hvad jeg sagde eeee \n",
      "\n",
      "It wasn't me who translated this text.  det var ikke hvad jeg sagde eeee \n",
      "\n",
      "It's a myth that poor people are lazy.  det er en af livets realiteter eeee \n",
      "\n",
      "It's not going to be a problem at all.  det vil ikke blive et problem eeee \n",
      "\n",
      "It's not going to be a problem at all.  det vil ikke blive et problem eeee \n",
      "\n",
      "It's only ten minutes' walk from here.  det er kun en del af problemet eeee \n",
      "\n",
      "It's the third biggest city of Serbia.  det er tid at regne i morgen eeee \n",
      "\n",
      "Kyoto was the former capital of Japan.  kassen var stolte af deres arbejde eeee \n",
      "\n",
      "Love is strong, but money is stronger.  uden luft er du fem eeee \n",
      "\n",
      "Man is the only animal that can laugh.  hvad er det en forkortelse for eeee \n",
      "\n",
      "Man is the only animal that uses fire.  hvad er det toms fødselsdag eeee \n",
      "\n",
      "Maria Callas is a famous opera singer.  filmen handler om en dreng ved navn eeee \n",
      "\n",
      "Maybe we should talk about this first.  tag venligst dette for givet tom eeee \n",
      "\n",
      "Money does not always bring happiness.  penge bringer ikke altid lykke eeee \n",
      "\n",
      "My brother died in a traffic accident.  min bror lavede en julekage til mig eeee \n",
      "\n",
      "My mother takes a nap every afternoon.  min mor har en masse til fælles eeee \n",
      "\n",
      "My older sister plays the guitar well.  min storebror ser tv eeee \n",
      "\n",
      "Only four horses competed in the race.  en af æblerne faldt på jorden eeee \n",
      "\n",
      "Our city is getting bigger and bigger.  vores klasse består på 40 drenge eeee \n",
      "\n",
      "Our hens laid a lot of eggs yesterday.  vores lærer er forsinket eeee \n",
      "\n",
      "Our hens laid a lot of eggs yesterday.  vores lærer er forsinket eeee \n",
      "\n",
      "Our hens laid a lot of eggs yesterday.  vores lærer er forsinket eeee \n",
      "\n",
      "Perhaps you'd be willing to volunteer.  måske skulle jeg gøre noget eeee \n",
      "\n",
      "Please make three copies of this page.  vær så venlig at lave en milkshake til mig eeee \n",
      "\n",
      "Please put this in the microwave oven.  vær så venlig at sætte mig som et barn eeee \n",
      "\n",
      "Santa Ana was living in exile in Cuba.  lad os tage toget eeee \n",
      "\n",
      "She advised him not to buy a used car.  hun tænker på ham en time eeee \n",
      "\n",
      "She fell in love with the new teacher.  hun faldt ned af stigen eeee \n",
      "\n",
      "She gave the children two apples each.  hun gav dem nogle æbler eeee \n",
      "\n",
      "She lived a quiet life in the country.  hun boede der i omkring et år eeee \n",
      "\n",
      "She put by some money for a rainy day.  hun har sendt mig en ring til mig eeee \n",
      "\n",
      "She spends as much money as she earns.  hun tjener hundrede euro om dagen eeee \n",
      "\n",
      "She told him that her father had died.  hun har født fem tusind yen eeee \n",
      "\n",
      "She was dressed in white at the party.  hun var fortørnet over at blive kaldt en kujon eeee \n",
      "\n",
      "Since when do you care about politics?  da jeg vågnede sneede det eeee \n",
      "\n",
      "Since when do you care about politics?  da jeg vågnede sneede det eeee \n",
      "\n",
      "Someone told me you left your husband.  til mary har været til boston nu eeee \n",
      "\n",
      "Someone told me you left your husband.  til mary har været til boston nu eeee \n",
      "\n",
      "Speak louder so everyone can hear you.  tal for dig selv eeee \n",
      "\n",
      "The Titanic sunk on its maiden voyage.  fuglen tog til london ved den eeee \n",
      "\n",
      "The accident almost cost him his life.  næsten bare være så klart som tom eeee \n",
      "\n",
      "The boy released a bird from the cage.  drengen svømmer en gang om ugen eeee \n",
      "\n",
      "The boy standing over there is my son.  drengen sad fast i mudderet eeee \n",
      "\n",
      "The circus will come to town tomorrow.  cirkusset kommer til at komme til sagen eeee \n",
      "\n",
      "The heavy rain kept us from going out.  regnen varede i tre dage eeee \n",
      "\n",
      "The heavy rain kept us from going out.  regnen varede i tre dage eeee \n",
      "\n",
      "The importance of music is underrated.  se alle så var min far eeee \n",
      "\n",
      "The lawyer decided to appeal the case.  besluttede at holde op med at ryge eeee \n",
      "\n",
      "The money came like manna from heaven.  æblet blev serveret som dessert eeee \n",
      "\n",
      "The next station is where you get off.  den gamle mand er på stolen eeee \n",
      "\n",
      "The nurse has taken my blood pressure.  sygeplejersken har efterladt sin pung derhjemme eeee \n",
      "\n",
      "The sidewalks were wet after the rain.  landsbyen var det pizarro slog ihjel eeee \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing on known sentences from validation data\n",
    "for idx in range(12000, 12100):\n",
    "    input_seq = input_texts[idx]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(input_seq, decoded_sentence, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "see you later  vi ses eeee \n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_seq = 'see you later'\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print(input_seq, decoded_sentence, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how are you  hvordan har du det eeee \n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_seq = 'how are you'\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print(input_seq, decoded_sentence, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
