{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with Attention\n",
    "- For now just reading files\n",
    "- Using my own tokenizer\n",
    "\n",
    "**Inspiration:**\n",
    "- https://distill.pub/2016/augmented-rnns/\n",
    "- https://arxiv.org/pdf/1511.04586v1.pdf\n",
    "- https://blog.heuritech.com/2016/01/20/attention-mechanism/\n",
    "- https://www.slideshare.net/KeonKim/attention-mechanisms-with-tensorflow\n",
    "- https://www.youtube.com/watch?v=ah7_mfl7LD0&t=4131s (minut 17)\n",
    "- https://www.youtube.com/watch?v=uuPZFWJ-4bE (minut 18:30)\n",
    "- http://www.manythings.org/anki/\n",
    "\n",
    "**Improvment ideas:**\n",
    "- Calculate BLEU Scores\n",
    "- Reduce \"num_words_src\" if found is less\n",
    "\n",
    "**Find out**\n",
    "- .call og .variables for objects: what do they do?\n",
    "\n",
    "**Illustration**\n",
    "The figure below illustrates to some extend how the attention mechanism works. The \"attention vector\" is calculated by adding the latest hidden output of target to each of the word output of the target. The best match gains the largest value. And this word gets the largest attention during translation:\n",
    "\n",
    "![title](attention.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "\n",
    "num_words_src = 5000       # Limit vocabulary in translation for source language\n",
    "num_words_tar = 5000       # Limit vocabulary in translation for target language\n",
    "\n",
    "dataSetSize = 16000        # small dataset = 16085, all data = 9999999\n",
    "truncate_std_div = 2       # truncate sentences after x tokens, 2 std dev = 95% included\n",
    "idx = 15000\n",
    "\n",
    "BATCH_SIZE = 64            # training batch size\n",
    "\n",
    "embedding_dim = 256        # Embedding dimensions\n",
    "GRU_units = 1024           # GRU dimension\n",
    "\n",
    "mark_start = 'ssss '       # start and end markes for destination sentences\n",
    "mark_end = ' eeee'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data into tables, small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of small dataset:  16085\n"
     ]
    }
   ],
   "source": [
    "# create lists for source and target texts\n",
    "source_texts_smallset = []\n",
    "target_texts_smallset = []\n",
    "\n",
    "# read file\n",
    "with open('dan-eng/dan.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    \n",
    "# split into source and target, add start and end marks\n",
    "for line in lines[:len(lines)-1]:\n",
    "    target_sentence, source_sentence = line.split('\\t')\n",
    "    target_sentence = mark_start + target_sentence.strip() + mark_end\n",
    "    source_texts_smallset.append(source_sentence)\n",
    "    target_texts_smallset.append(target_sentence)\n",
    "\n",
    "print('Size of small dataset: ', len(source_texts_smallset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of small dataset:  16085\n",
      "Hvad har Tom for?\n",
      "ssss What's Tom up to? eeee\n"
     ]
    }
   ],
   "source": [
    "print('Size of small dataset: ', len(source_texts_smallset))\n",
    "print(source_texts_smallset[2000])\n",
    "print(target_texts_smallset[2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data into tables, large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_texts = []\n",
    "target_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source into a table, the second and larger dataset\n",
    "filename = \"europarl-v7.da-en.da\"\n",
    "data_dir = \"data/europarl/\"\n",
    "path = os.path.join(data_dir, filename)\n",
    "with open(path, encoding=\"utf-8\") as file:\n",
    "    # Read the line from file, strip leading and trailing whitespace,\n",
    "    # prepend the start-text and append the end-text.\n",
    "    source_texts = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# destination into a table, the second and larger dataset\n",
    "filename = \"europarl-v7.da-en.en\"\n",
    "path = os.path.join(data_dir, filename)\n",
    "with open(path, encoding=\"utf-8\") as file:\n",
    "    # Read the line from file, strip leading and trailing whitespace,\n",
    "    # prepend the start-text and append the end-text.\n",
    "    target_texts = [mark_start + line.strip() + mark_end for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of large dataset:  1968800\n",
      "Jeg er sikker på, at vi vil få andre ansøgninger om optagelse, og at dette skønne arbejde, dette skønne intellektuelle bygningsværk derfor vil styrte sammen, fordi det bliver overhalet af begivenheder, sådan som vi i nu 20 år er blevet overhalet af alt det, der er sket i det tidligere Jugoslavien.\n",
      "ssss There are bound to be other applications and this great work, this fine intellectual architecture, will crumble, left behind by events just as we have been left behind by everything that has been happening in the former Yugoslavia for the past 20 years. eeee\n"
     ]
    }
   ],
   "source": [
    "print('Size of large dataset: ', len(source_texts))\n",
    "print(source_texts[idx])\n",
    "print(target_texts[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the two datasets ... and limit data set size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of small+large dataset:  1984885\n"
     ]
    }
   ],
   "source": [
    "# join the two data set to one big, gives me both short and long sentences\n",
    "source_texts = source_texts_smallset + source_texts\n",
    "target_texts = target_texts_smallset + target_texts\n",
    "print('Size of small+large dataset: ', len(source_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size:    1984885 1984885\n",
      "New lighter dataset size: 16000 16000\n"
     ]
    }
   ],
   "source": [
    "# shorten data sets to speed up training for easy experimentation\n",
    "print('Original dataset size:   ', len(source_texts), len(target_texts))\n",
    "source_texts = source_texts[:dataSetSize]\n",
    "target_texts = target_texts[:dataSetSize]\n",
    "print('New lighter dataset size:', len(source_texts), len(target_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source texts</th>\n",
       "      <th>Target texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9920</th>\n",
       "      <td>Tom ønskede at se Marys værelse.</td>\n",
       "      <td>ssss Tom wanted to see Mary's room. eeee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6121</th>\n",
       "      <td>Jeg plejer at tage mig af opvasken.</td>\n",
       "      <td>ssss I usually do the dishes. eeee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7463</th>\n",
       "      <td>Mary er en oprørsk pige.</td>\n",
       "      <td>ssss Mary is a rebellious girl. eeee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11096</th>\n",
       "      <td>Han spiller golf, selvom det regner.</td>\n",
       "      <td>ssss He'll play golf even if it rains. eeee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4125</th>\n",
       "      <td>Jeg kan lide at spise æbler.</td>\n",
       "      <td>ssss I like to eat apples. eeee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Source texts  \\\n",
       "9920   Tom ønskede at se Marys værelse.       \n",
       "6121   Jeg plejer at tage mig af opvasken.    \n",
       "7463   Mary er en oprørsk pige.               \n",
       "11096  Han spiller golf, selvom det regner.   \n",
       "4125   Jeg kan lide at spise æbler.           \n",
       "\n",
       "                                      Target texts  \n",
       "9920   ssss Tom wanted to see Mary's room. eeee     \n",
       "6121   ssss I usually do the dishes. eeee           \n",
       "7463   ssss Mary is a rebellious girl. eeee         \n",
       "11096  ssss He'll play golf even if it rains. eeee  \n",
       "4125   ssss I like to eat apples. eeee              "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot some examples\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "df = pd.DataFrame({'Source texts':source_texts, 'Target texts':target_texts})\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize source sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7296 unique source tokens.\n",
      "Longest sentence is 18 tokens.\n",
      "Sentences shortened to max 10 tokens.\n",
      "Shape of source tokens: (16000, 10)\n",
      "Source example:\n",
      "As tokens:     [  6   1  18 352   7 129  55 950   0   0]\n",
      "As recreated:  det er et problem du selv må løse\n",
      "As original:   Det er et problem du selv må løse.\n"
     ]
    }
   ],
   "source": [
    "# crate source tokenizer and create vocabulary from the texts\n",
    "tokenizer_src = Tokenizer(num_words=num_words_src)\n",
    "tokenizer_src.fit_on_texts(source_texts)\n",
    "print('Found %s unique source tokens.' % len(tokenizer_src.word_index))\n",
    "\n",
    "# translate from word sentences to token sentences\n",
    "tokens_src = tokenizer_src.texts_to_sequences(source_texts)\n",
    "\n",
    "# Reverse the token-sequences\n",
    "# tokens_inp = [list(reversed(x)) for x in tokens_inp]\n",
    "\n",
    "# Shorten the longest token sentences, \n",
    "# Find the length of all sentences, truncate after x * std deviations\n",
    "num_tokens = [len(x) for x in tokens_src]\n",
    "print('Longest sentence is %s tokens.' % max(num_tokens))\n",
    "max_tokens_src = np.mean(num_tokens) + truncate_std_div * np.std(num_tokens)\n",
    "max_tokens_src = min(int(max_tokens_src), max(num_tokens))\n",
    "print('Sentences shortened to max %s tokens.' % max_tokens_src)\n",
    "\n",
    "# Pad / truncate all token-sequences to the given length\n",
    "tokens_padded_src = pad_sequences(tokens_src,\n",
    "                                  maxlen=max_tokens_src,\n",
    "                                  padding='post',\n",
    "                                  truncating='post')\n",
    "\n",
    "# Create inverse lookup from integer-tokens to words\n",
    "index_to_word_src = dict(zip(tokenizer_src.word_index.values(), \n",
    "                             tokenizer_src.word_index.keys()))\n",
    "\n",
    "# function to return readable text from tokens string\n",
    "def tokens_to_string_src(tokens):\n",
    "    words = [index_to_word_src[token] \n",
    "            for token in tokens\n",
    "            if token != 0]\n",
    "    text = \" \".join(words)\n",
    "    return text\n",
    "\n",
    "# demo to show that it works\n",
    "print('Shape of source tokens:', tokens_padded_src.shape)\n",
    "print('Source example:')\n",
    "print('As tokens:    ', tokens_padded_src[idx])\n",
    "print('As recreated: ', tokens_to_string_src(tokens_padded_src[idx]))\n",
    "print('As original:  ', source_texts[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize target sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5438 unique target tokens.\n",
      "Longest sentence is 18 tokens.\n",
      "Sentences shortened to max 12 tokens.\n",
      "Shape of target tokens: (16000, 12)\n",
      "Target example:\n",
      "As tokens:     [  1  17   8   5 220   7  13   9 903  81 414   2]\n",
      "As recreated:  ssss this is a problem you have to solve by yourself eeee\n",
      "As original:   ssss This is a problem you have to solve by yourself. eeee\n"
     ]
    }
   ],
   "source": [
    "# crate source tokenizer and create vocabulary from the texts\n",
    "tokenizer_tar = Tokenizer(num_words=num_words_tar)\n",
    "tokenizer_tar.fit_on_texts(target_texts)\n",
    "print('Found %s unique target tokens.' % len(tokenizer_tar.word_index))\n",
    "\n",
    "# translate from word sentences to token sentences\n",
    "tokens_tar = tokenizer_tar.texts_to_sequences(target_texts)\n",
    "\n",
    "# Shorten the longest token sentences, \n",
    "# Find the length of all sentences, truncate after x * std deviations\n",
    "num_tokens = [len(x) for x in tokens_tar]\n",
    "print('Longest sentence is %s tokens.' % max(num_tokens))\n",
    "max_tokens_tar = np.mean(num_tokens) + truncate_std_div * np.std(num_tokens)\n",
    "max_tokens_tar = min(int(max_tokens_tar), max(num_tokens))\n",
    "print('Sentences shortened to max %s tokens.' % max_tokens_tar)\n",
    "\n",
    "# Pad / truncate all token-sequences to the given length\n",
    "tokens_padded_tar = pad_sequences(tokens_tar,\n",
    "                                  maxlen=max_tokens_tar,\n",
    "                                  padding='post',\n",
    "                                  truncating='post')\n",
    "\n",
    "# Create inverse lookup from integer-tokens to words\n",
    "index_to_word_tar = dict(zip(tokenizer_tar.word_index.values(), \n",
    "                             tokenizer_tar.word_index.keys()))\n",
    "\n",
    "# function to return readable text from tokens string\n",
    "def tokens_to_string_tar(tokens):\n",
    "    words = [index_to_word_tar[token] \n",
    "            for token in tokens\n",
    "            if token != 0]\n",
    "    text = \" \".join(words)\n",
    "    return text\n",
    "\n",
    "# demo to show that it works\n",
    "print('Shape of target tokens:', tokens_padded_tar.shape)\n",
    "print('Target example:')\n",
    "print('As tokens:    ', tokens_padded_tar[idx])\n",
    "print('As recreated: ', tokens_to_string_tar(tokens_padded_tar[idx]))\n",
    "print('As original:  ', target_texts[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n"
     ]
    }
   ],
   "source": [
    "# start and end marks as tokens, needed when translating\n",
    "token_start = tokenizer_tar.word_index[mark_start.strip()]\n",
    "token_end =   tokenizer_tar.word_index[mark_end.strip()]\n",
    "print(token_start, token_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(tokens_padded_src)\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((tokens_padded_src, tokens_padded_tar)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Encoder Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.CuDNNGRU(enc_units, \n",
    "                                            return_sequences=True, \n",
    "                                            return_state=True, \n",
    "                                            recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Decoder object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.CuDNNGRU(dec_units, \n",
    "                                            return_sequences=True, \n",
    "                                            return_state=True, \n",
    "                                            recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V  = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        \n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "           \n",
    "        # ----------------------------- ATTENTION !\n",
    "    \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # -----------------------------\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "            \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(num_words_src, embedding_dim, GRU_units, BATCH_SIZE)\n",
    "decoder = Decoder(num_words_tar, embedding_dim, GRU_units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = 1 - np.equal(real, 0)\n",
    "  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.7356\n",
      "Epoch 1 Batch 100 Loss 2.9247\n",
      "Epoch 1 Batch 200 Loss 2.5249\n",
      "Epoch 1 Loss 2.9388\n",
      "Time taken for 1 epoch 35.535956382751465 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.3019\n",
      "Epoch 2 Batch 100 Loss 2.4626\n",
      "Epoch 2 Batch 200 Loss 1.9072\n",
      "Epoch 2 Loss 2.1648\n",
      "Time taken for 1 epoch 35.72203516960144 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.7333\n",
      "Epoch 3 Batch 100 Loss 1.4646\n",
      "Epoch 3 Batch 200 Loss 1.4315\n",
      "Epoch 3 Loss 1.5038\n",
      "Time taken for 1 epoch 34.69463777542114 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.0534\n",
      "Epoch 4 Batch 100 Loss 1.1523\n",
      "Epoch 4 Batch 200 Loss 0.9760\n",
      "Epoch 4 Loss 1.0431\n",
      "Time taken for 1 epoch 35.240270376205444 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.7859\n",
      "Epoch 5 Batch 100 Loss 0.7961\n",
      "Epoch 5 Batch 200 Loss 0.8847\n",
      "Epoch 5 Loss 0.7534\n",
      "Time taken for 1 epoch 35.15317153930664 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.6231\n",
      "Epoch 6 Batch 100 Loss 0.5848\n",
      "Epoch 6 Batch 200 Loss 0.5742\n",
      "Epoch 6 Loss 0.5538\n",
      "Time taken for 1 epoch 35.021015882492065 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.4909\n",
      "Epoch 7 Batch 100 Loss 0.4023\n",
      "Epoch 7 Batch 200 Loss 0.4206\n",
      "Epoch 7 Loss 0.4201\n",
      "Time taken for 1 epoch 34.39438056945801 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.2585\n",
      "Epoch 8 Batch 100 Loss 0.3098\n",
      "Epoch 8 Batch 200 Loss 0.3576\n",
      "Epoch 8 Loss 0.3265\n",
      "Time taken for 1 epoch 34.80943202972412 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.2567\n",
      "Epoch 9 Batch 100 Loss 0.1992\n",
      "Epoch 9 Batch 200 Loss 0.2793\n",
      "Epoch 9 Loss 0.2508\n",
      "Time taken for 1 epoch 34.328688859939575 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.1438\n",
      "Epoch 10 Batch 100 Loss 0.2024\n",
      "Epoch 10 Batch 200 Loss 0.1970\n",
      "Epoch 10 Loss 0.1967\n",
      "Time taken for 1 epoch 34.8037383556366 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)    # enc_output is 64x 10x1024\n",
    "                                                             # enc_hidden is 64x    1024\n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            # 64x1 tensor with ssss\n",
    "            dec_input = tf.expand_dims([token_start] * BATCH_SIZE, 1)       \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):                # targ is 64x12, ie for 12 words\n",
    "                \n",
    "                # passing enc_output to the decoder, predictions=64x5000, dec_hidden=64x1024\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that does the translation\n",
    "\n",
    "def translate_sequence(input_seq):\n",
    "    \n",
    "    result = ''\n",
    "    attention_plot = np.zeros((num_words_tar, num_words_src))\n",
    "    \n",
    "    # tokenize the text to be translated, pad and convert tto tensor\n",
    "    input_tokens = tokenizer_src.texts_to_sequences([input_seq])\n",
    "    input_tokens = pad_sequences(input_tokens,\n",
    "                                 maxlen=max_tokens_src,\n",
    "                                 padding='post',\n",
    "                                 truncating='post')\n",
    "    input_tokens = tf.convert_to_tensor(input_tokens)\n",
    "    \n",
    "    # run encoder on input sentence\n",
    "    hidden = [tf.zeros((1, GRU_units))]\n",
    "    enc_out, enc_hidden = encoder(input_tokens, hidden)\n",
    "    \n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([token_start], 0)\n",
    " \n",
    "    for t in range(max_tokens_tar):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weigths to plot later on\n",
    "  #      attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "  #      attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += index_to_word_tar[predicted_id] + ' ' \n",
    "\n",
    "        if index_to_word_tar[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, input_seq, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hi with you eeee eeee eeee eeee eeee eeee eeee eeee eeee ',\n",
       " 'hej med dig',\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_sequence('hej med dig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word2idx[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weigths to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.idx2word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.idx2word[predicted_id] == 'eeee':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "        \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x2adca827518>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inp_lang' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-f3af96da161c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hvad hedder du'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp_lang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarg_lang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length_inp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length_targ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'inp_lang' is not defined"
     ]
    }
   ],
   "source": [
    "translate('hvad hedder du', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tester ..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# print(tensor.get_shape())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "epoch = 0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hidden = encoder.initialize_hidden_state()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hidden"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "total_loss = 0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batch = 0\n",
    "(inp, targ) = dataset.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "loss = 0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "enc_output, enc_hidden = encoder(inp, hidden)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dec_hidden = enc_hidden"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dec_input = tf.expand_dims([token_start] * BATCH_SIZE, 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "t = 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "loss += loss_function(targ[:, t], predictions)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "targ[:, t]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dec_input"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dec_input = tf.expand_dims(targ[:, t], 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dec_input"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "t+=1\n",
    "t"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "targ.shape[1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batch_loss = (loss / int(targ.shape[1]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batch_loss"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "total_loss += batch_loss\n",
    "total_loss"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "variables = encoder.variables + decoder.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
